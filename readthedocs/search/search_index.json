{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Template usage \u00b6 \u4e00\u4e2areadthedoc\u7b80\u6613\u6a21\u7248\uff0c\u76ee\u6807\u662f\u540c\u6b65bookdown\u505a\u4e24\u4e2a\u7c7b\u4f3c\u7684\u7f51\u9875\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u5185\u5bb9\uff0c\u56e0\u4e3a\u770b\u89c1\u6b64\u6a21\u7248\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528markdown\u6587\u4ef6\uff0c\u611f\u89c9\u662f\u53ef\u884c\u7684\u3002 How to use \u00b6 \u5982\u4f55\u4f7f\u7528\uff1a\u5728 mkdocs.yml \u6587\u4ef6\u91cc\u9762\uff0c\u6709 nav: - Home: - 'Home': index.md #\u4e3b\u9875 \u5176\u4ed6\u7684\u53ef\u4ee5\u5148\u4e0d\u7ba1 - 'Before FOSS Starts': installation.md - 'Schedule': schedule.md - 'Code of Conduct': code_of_conduct.md - 'Glossary & Acronyms': glossary.md - Lessons: - '0. The Shell and Git': 00_basics.md - '1. Open Science': 01_intro_open_sci.md - '2. Project Management': 02_project_management.md - '3. Managing Data': 03_managing_data.md - '4. Documentation and Communication': 04_documentation_communication.md - '5. Version Control': 05_version_control.md - '6. Reproducibility I: Repeatability': 06_reproducibility_i.md - '7. Reproducibility II: Containers': 07_reproducibility_ii.md - Capstone Project: - 'Overview': final_project/overview.md \u4e3a\u4e86\u4f7f markdown \u5728bookdown\u548creadthedoc\u90fd\u80fd\u8fd0\u884c\uff0c\u6211\u8fd9\u91cc\u5c31\u6253\u7b97\u53bb\u6389\u6587\u4ef6\u5939\u91cc\u9762\u7684\u5185\u5bb9\uff0c\u6216\u8005\u52a0\u4e00\u4e2a\u81ea\u52a8\u8bc6\u522b\u4e4b\u540e\u3002","title":"Home"},{"location":"#template-usage","text":"\u4e00\u4e2areadthedoc\u7b80\u6613\u6a21\u7248\uff0c\u76ee\u6807\u662f\u540c\u6b65bookdown\u505a\u4e24\u4e2a\u7c7b\u4f3c\u7684\u7f51\u9875\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u5185\u5bb9\uff0c\u56e0\u4e3a\u770b\u89c1\u6b64\u6a21\u7248\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528markdown\u6587\u4ef6\uff0c\u611f\u89c9\u662f\u53ef\u884c\u7684\u3002","title":"Template usage"},{"location":"#how-to-use","text":"\u5982\u4f55\u4f7f\u7528\uff1a\u5728 mkdocs.yml \u6587\u4ef6\u91cc\u9762\uff0c\u6709 nav: - Home: - 'Home': index.md #\u4e3b\u9875 \u5176\u4ed6\u7684\u53ef\u4ee5\u5148\u4e0d\u7ba1 - 'Before FOSS Starts': installation.md - 'Schedule': schedule.md - 'Code of Conduct': code_of_conduct.md - 'Glossary & Acronyms': glossary.md - Lessons: - '0. The Shell and Git': 00_basics.md - '1. Open Science': 01_intro_open_sci.md - '2. Project Management': 02_project_management.md - '3. Managing Data': 03_managing_data.md - '4. Documentation and Communication': 04_documentation_communication.md - '5. Version Control': 05_version_control.md - '6. Reproducibility I: Repeatability': 06_reproducibility_i.md - '7. Reproducibility II: Containers': 07_reproducibility_ii.md - Capstone Project: - 'Overview': final_project/overview.md \u4e3a\u4e86\u4f7f markdown \u5728bookdown\u548creadthedoc\u90fd\u80fd\u8fd0\u884c\uff0c\u6211\u8fd9\u91cc\u5c31\u6253\u7b97\u53bb\u6389\u6587\u4ef6\u5939\u91cc\u9762\u7684\u5185\u5bb9\uff0c\u6216\u8005\u52a0\u4e00\u4e2a\u81ea\u52a8\u8bc6\u522b\u4e4b\u540e\u3002","title":"How to use"},{"location":"00_basics/","text":"Intro \u00b6 Some intro Requirements \u00b6 Software requirements Usage \u00b6 one 2 3) xxx","title":"Intro"},{"location":"00_basics/#intro","text":"Some intro","title":"Intro"},{"location":"00_basics/#requirements","text":"Software requirements","title":"Requirements"},{"location":"00_basics/#usage","text":"one 2 3) xxx","title":"Usage"},{"location":"01_intro_open_sci/","text":"Introduction to Open Science \u00b6 Learning Objectives After this lesson, you should be able to: Explain what Open Science is Explain the components of Open Science Describe the behaviors of Open Science Explain why Open Science matters in education, research, and society Understand the advantages and the challenges to Open Science Identify who the practitioners of Open Science are Understand the underlying Ethos of Open Science What is Open Science? \u00b6 If you ask a dozen researchers this question, you will probably get just as many answers. This means that Open Science isn't necessarily a set of checkboxes you need to tick, but rather a holistic approach to doing science. Definitions \"Open Science is defined as an inclusive construct that combines various movements and practices aiming to make multilingual scientific knowledge openly available, accessible and reusable for everyone, to increase scientific collaborations and sharing of information for the benefits of science and society, and to open the processes of scientific knowledge creation, evaluation and communication to societal actors beyond the traditional scientific community.\" - UNESCO Definition UNESCO's Recommendation on Open Science \"Open Science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional...\" Wikipedia definition Open and Collaborative Science Network's Open Science Manifesto Six Pillars of Open Science Open Access Publications Open Data Open Educational Resources Open Methodology Open Peer Review Open Source Software Wait, how many pillars of Open Science Really Are There? The number can be from 4 to 8 Graphic by Foster Open Science flowchart LR id1([open science]) --> id3([publishing]) & id4([data]) & id5([open source software]) id3([publishing]) --> id41([access]) & id42([reviews]) & id43([methods]) & id44([educational resources]) id5([open source software]) --> id13([container registries]) & id10([services]) & id101([workflows]) & id12([version control systems]) id12([version control systems]) --> id101([workflows]) id13([container registries]) --> id101([workflows]) id14([public data registry]) --> id101([workflows]) id10([services]) --> id101([workflows]) id44([educational resources]) --> id21([university libraries]) id21([university libraries]) --> id101([workflows]) id22([federal data archives]) --> id101([workflows]) id4([data]) --> id21([university libraries]) & id22([federal data archives]) & id14([public data registries]) id101([workflows]) --> id15([on-premises]) & id16([commercial cloud]) & id17([public cloud]) Mermaid Diagram: Conceptual relationships of Open Science and cyberinfrastructure Awesome Lists of Open Science Awesome lists were started on GitHub by Sindre Sorhus and typically have a badge associated with them (There is even a Searchable Index of Awesome Lists) We have created our own Awesome Open Science List here which may be valuable to you. Open Access Publications \u00b6 Definitions \"Open access is a publishing model for scholarly communication that makes research information available to readers at no cost, as opposed to the traditional subscription model in which readers have access to scholarly information by paying a subscription (usually via libraries).\" -- OpenAccess.nl Pre-print Services ASAPbio Pre-Print Server List - ASAPbio is a scientist-driven non-profit promoting transparency and innovation comprehensive list of pre-print servers inthe field of life science communication. ESSOar - Earth and Space Science Open Archive hosted by the American Geophysical Union. Peer Community In (PCI) a free recommendation process of scientific preprints based on peer reviews OSF.io Preprints are partnered with numerous projects under the \"-rXivs\" The rXivs AfricArXiv AgrirXiv Arabixiv arXiv - is a free distribution service and an open-access archive for 2,086,431 scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics. BioHackrXiv BioRxiv - is an open access preprint repository for the biological sciences. BodorXiv EarthArXiv - is an open access preprint repository for the Earth sciences. EcsArXiv - a free preprint service for electrochemistry and solid state science and technology EdArXiv - for the education research community EngrXiv for the engineering community EvoEcoRxiv - is an open acccess preprint repository for Evolutionary and Ecological sciences. MediArXiv for Media, Film, & Communication Studies MedRxiv - is an open access preprint repository for Medical sciences. PaleorXiv - is an open access preprint repository for Paleo Sciences PsyrXiv - is an open access preprint repository for Psychological sciences. SocArXiv - is an open access preprint repository for Social sciences. SportrXiv - is an open access preprint for Sports sciences. ThesisCommons - open Theses Financial Support for Open Access Publishing Fees There are mechanisms for helping to pay for the additional costs of publishing research as open access: SciDevNet Health InterNetwork Access to Research Initiative (HINARI) Some institutions offer support for managing publishing costs (check to see if your institution has such support): University of Arizona Open Access Investment Fund Colorado University at Boulder Open Access Fund Max Planck Digital Library - German authors can have OA fees in Springer Nature research journals paid for. Bibsam Consortium - Swedish authors can have OA fees in Springer Nature research journals paid for. Open Access Publishing Major publishers have provided access points for publishing your work AAAS Nature American Geophysical Union Commonwealth Scientific and Industrial Research Organisation (CSIRO) Open Research Europe Open Data \u00b6 Open Data are a critical aspect of open science. There are three key attributes of Open Data: Availability and accessibility Reusability Inclusivity Definitions \u201cOpen data and content can be freely used, modified, and shared by anyone for any purpose\u201d - The Open Definition \"Open data is data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and sharealike.\" - Open Data Handbook Wikipedia definition DIKW Pyramid Data are the basis of our understanding the natural world. The Data-Information-Knowledge-Wisdom (DIKW) pyramid describes for us how data are refined into information and knowledge. FAIR & CARE Principles Wilkinson et al. (2016) established the guidelines to improve the Findability, Accessibility, Interoperability, and Reuse (FAIR) of digital assets for research. Go-FAIR website Carroll et al. (2020) established the CARE Principles for Indigenous Data Governance. full document Indigenous Data Sovereignty Networks LOD Cloud The Linked Open Data Cloud shows how data are linked to one another forming the basis of the semantic web . Open Educational Resources \u00b6 Definitions \"Open Educational Resources (OER) are learning, teaching and research materials in any format and medium that reside in the public domain or are under copyright that have been released under an open license, that permit no-cost access, re-use, re-purpose, adaptation and redistribution by others.\" - UNESCO Wikipedia definition Digital Literacy Organizations The Carpentries - teaches foundational coding and data science skills to researchers worldwide EdX - Massively Online Online Courses (not all open) hosted through University of California Berkeley EveryoneOn - mission is to unlock opportunity by connecting families in underserved communities to affordable internet service and computers, and delivering digital skills trainings ConnectHomeUSA - is a movement to bridge the digital divide for HUD-assisted housing residents in the United States under the leadership of national nonprofit EveryoneOn Global Digital Literacy Council - has dedicated more than 15 years of hard work to the creation and maintenance of worldwide standards in digital literacy IndigiData - training and engaging tribal undergraduate and graduate students in informatics National Digital Equity Center a 501c3 non-profit, is a nationally recognized organization with a mission to close the digital divide across the United States National Digital Inclusion Allaince - advances digital equity by supporting community programs and equipping policymakers to act Net Literacy Open Educational Resources Commons Project Pythia is the education working group for Pangeo and is an educational resource for the entire geoscience community Research Bazaar - is a worldwide festival promoting the digital literacy emerging at the centre of modern research TechBoomers - is an education and discovery website that provides free tutorials of popular websites and Internet-based services in a manner that is accessible to older adults and other digital technology newcomers Educational Materials Teach Together by Greg Wilson DigitalLearn Open Methodology \u00b6 The use of version control systems like GitHub and GitLab present one of the foremost platforms for sharing open methods for digital research. Definitions \"An open methodology is simply one which has been described in sufficient detail to allow other researchers to repeat the work and apply it elsewhere.\" - Watson (2015) \"Open Methodology refers to opening up methods that are used by researchers to achieve scientific results and making them publicly available.\" - Open Science Network Austria Protocols and Bench Techniques BioProtocol Current Protocols Gold Biotechnology Protocol list JoVE - Journal of Visualized Experiments Nature Protocols OpenWetWare Protocol Exchange Protocols Online Protocols SciGene Springer Nature Experiments Open Peer Review \u00b6 Pros and Cons of Open Peer Review Definitions Ross-Hellauer et al. (2017) ask What is Open Peer Review? and state that there is no single agreed upon definition Wikipedia's definition Open Peer Review Resources F1000Research the first open research publishing platform. Offering open peer review rapid publication PREreview provides a space for open peer reviews, targeted toward early career researchers. ASAPbio Accelerating Science and Publication in Biology, an open peer review source for biologists and life scientists. PubPeer platform for post-publication of peer reviews. Sciety platform for evaluating preprints. Open Source Software \u00b6 Definitions \"Open source software is code that is designed to be publicly accessible\u2014anyone can see, modify, and distribute the code as they see fit. Open source software is developed in a decentralized and collaborative way, relying on peer review and community production.\" - Red Hat Open Source Initiative definition Wikipedia definition Awesome list Breakout Discussion \u00b6 As you already know, being a scientist requires you to wear many hats, and trying to do Open Science is no different. Bernery et al. (2022) Figure 2: The positive aspects of doing a PhD. As we mentioned, Open Science is not a set of boxes you need to check off to be \"Certified Open\", but an intersecting set of philosophies and approaches, all of which occur on some type of spectrum. To get a feel for how Open Science can be multifaceted and different for each researcher, we will do a short breakout group session to discuss what Open Science means to you. What does Open Science mean to you? Which of the pillars of Open Science is nearest to your own heart? Open Access Publications Open Data Open Educational Resources Open Methodology Open Peer Review Open Source Software Are any of the pillars more important than the others? Are there any pillars not identified that you think should be considered? Components of Open Science \u00b6 One of the most fundamental, and certainly the most publicized component of Open Science is the accessibility of data. This makes sense - without access to your data, nothing else about your science can be all that open. While we will devote an entire 10 weeks of this course to data, opening up your data is only one piece of the puzzle. Gallagher et al (2020) Box 2: six core principles of Open Science. This figure demonstrates the multiple intersecting pieces of Open Science, which go beyond simply making data accessible. While we focus primarily on Open Data, Open Source, and Open Methodology in FOSS, it's worth considering how other parts of the scientific process might be opened up more broadly. Another component which sort of covers all of the pictured components, or at least links a lot of them together, might be referred to as Open Process. In response to the Reproducibility Crisis, many researchers, particularly in fields like psychology, have begun to advocate for preregistration of studies. This involves writing out and publishing your entire research plan, from data collection to analysis and publication, for the sake of avoiding practices like p-hacking or HARKing . What preregistration also does is make the process of your work more open, including many of the small decisions and tweaks you make to a project that probably wouldn't make it into a manuscript. To learn more about preregistration, you can check out the Open Science Foundation , a project that provides a preregistration platform and other Open Science tools. As mentioned above, it is worthwhile to think about Open Science not as a set of checkboxes, but rather a holistic approach to doing science. In that spirit, it can also be useful to think about Open Science as a spectrum, from less to more open. While you might not achieve some platonic ideal of openness for a variety of reasons, you can still make great progress in moving your science towards the Open end of the spectrum. In reality, a large scientific project probably consists of multiple spectra; you can move your data towards the open end of the spectrum while your software remains less open, and vice versa. All this is to say that doing Open Science is not a static set of goals you must achieve, it is a process that grows and changes with your science itself. One of the biggest challenges of doing science is that you might have to wear many different hats: domain expert, lab manager, statistician, teacher, mentor, grant writer, manuscript author, public speaker... the list goes on. Doing Open Science is no different, but the list of skills may be even greater, since the goal is now to openly communicate each step of the process to a broader audience. This also makes teaching Open Science quite challenging- we will cover topics ranging from \"soft skills\" like project management and internal communications to more technical skills like software management and containers. We could probably teach this whole workshop on each single topic, but we clearly don't have the time to do that. Instead, we will focus on a higher-level look at the landscape of Open Science and introduce you to a wide variety of skills and concepts with the idea that you can go on to find ways to implement them in your own work. What characteristics might a paper, project, lab group require to qualify as doing Open Science What are some limitations to you, your lab group, or your domain? WHY do Open Science? \u00b6 There are many reasons to do Open Science, and presumably one or more of them brought you to this workshop. Whether you feel an ethical obligation, want to improve the quality of your work, or want to look better to funding agencies, many of the same approaches to Open Science apply. A paper from Bartling & Friesike (2014) posits that there are 5 main schools of thought in Open Science, which represent 5 underlying motivations: Democratic school : primarily concerned with making scholarly work freely available to everyone Pragmatic school : primarily concerned with improving the quality of scholarly work by fostering collaboration and improving critiques Infrastructure school : primarily focused on the platforms, tools, and services necessary to conduct efficient research, collaboration, and communication Public school : primarily concerned with societal impact of scholarly work, focusing on engagement with broader public via citizen science, understandable scientific communication, and less formal communication Measurement school : primarily concerned with the existing focus on journal publications as a means of measuring scholarly output, and focused on developing alternative measurements of scientific impact In Bartling & Friesike (2014) Open Science: One Term, Five Schools of Thought While many researchers may be motivated by one or more of these aspects, we will not necessarily focus on any of them in particular. If anything, FOSS may be slightly more in the Infrastructure school, because we aim to give you the tools to do Open Science based on your own underlying motivations. Let's break out into groups again to discuss some of our motivations for doing Open Science. What motivates you to do Open Science? Do you feel that you fall into a particular \"school\"? If so, which one, and why? Are there any motivating factors for doing Open Science that don't fit into this framework? Ethos of Open Science \u00b6 Doing Open Science requires us to understand the ethics of why working with data which do not belong to us is privileged. We must also anticipate how these could be re-used in ways contrary to the interests of humanity . Ensure the use of Institutional Review Boards (IRB) or your local ethical committee. Areas to consider: Source: UK Statistics Authority Geolocation (survey, land ownership, parcel data), see UK Statistics Authority Ethical Considerations Personal identification information US Personal Identifiable Information (PII) , General Data Protection Regulation (GDPR) Health information US HIPAA , EU GDPR Protected and Endangered Species ( US Endangered Species Act ) Indigenous data sovereignty: CARE Principles for Indigenous Data Governance , Global Indigenous Data Alliance (GIDA) , First Nations OCAP\u00ae (Ownership Control Access and Possession) , Circumpolar Inuit Protocols for Equitable and Ethical Engagement Artificial intelligence/machine learning Assessment List Trustworthy AI (ALTAI) from the European AI Alliance \"Nothing about us, without us\" Funnel et al. (2019) For more information (training): Ethics and Data Access (General Information with BioMedical and Life Sciences Data) includes a legal and ethical checklist lesson for researchers around \"FAIR Plus\". Recommended Open Science Communities \u00b6 Open Scholarship Grassroots Community Networks International Open Science Networks Center for Scientific Collaboration and Community Engagement (CSCCE) Center for Open Science (COS) Eclipse Science Working Group eLife NumFocus Open Access Working Group Open Research Funders Group Open Science Foundation Open Science Network pyOpenSci R OpenSci Research Data Alliance (RDA) The Turing Way UNESCO Global Open Science Partnership World Wide Web Consortium (W3C) US-based Open Science Networks CI Compass - provides expertise and active support to cyberinfrastructure practitioners at USA NSF Major Facilities in order to accelerate the data lifecycle and ensure the integrity and effectiveness of the cyberinfrastructure upon which research and discovery depend. Earth Science Information Partners (ESIP) Federation - is a 501\u00a9(3) nonprofit supported by NASA, NOAA, USGS and 130+ member organizations. Internet2 - is a community providing cloud solutions, research support, and services tailored for Research and Education. Minority Serving Cyberinfrastructure Consortium (MS-CC) envisions a transformational partnership to promote advanced cyberinfrastructure (CI) capabilities on the campuses of Historically Black Colleges and Universities (HBCUs), Hispanic-Serving Institutions (HSIs), Tribal Colleges and Universities (TCUs), and other Minority Serving Institutions (MSIs). NASA Transform to Open Science (TOPS) - coordinates efforts designed to rapidly transform agencies, organizations, and communities for Earth Science OpenScapes - is an approach for doing better science for future us The Quilt - non-profit regional research and education networks collaborate to develop, deploy and operate advanced cyberinfrastructure that enables innovation in research and education. Oceania Open Science Networks New Zealand Open Research Network - New Zealand Open Research Network (NZORN) is a collection of researchers and research-associated workers in New Zealand. Australia & New Zealand Open Research Network - ANZORN is a network of local networks distributed without Australia and New Zealand. Self Assessment \u00b6 True or False: All research papers published in the top journals, like Science and Nature, are always Open Access? Failure False Major Research journals like Science and Nature have an \"Open Access\" option when a manuscript is accepted, but they charge an extra fee to the authors to make those papers Open Access. These high page costs are exclusionary to the majority of global scientists who cannot afford to front these costs out of pocket. This will soon change, at least in the United States. The Executive Branch of the federal government recently mandated that future federally funded research be made Open Access after 2026. True or False: an article states all of the research data used in the experiments \"are available upon request from the corresponding author(s),\" meaning the data are \"Open\" Failure False In order for research to be open, the data need to be freely available from a digital repository, like Data Dryad , Zenodo.org , or CyVerse . Data that are 'available upon request' do not meet the FAIR data principles. True or False: Online Universities and Data Science Boot Camps like UArizona Online, Coursera, Udemy, etc. promote digital literacy and are Open Educational Resources? Failure False These examples are for-profit programs which teach data science and computer programming online. Some may be official through public or private universities and offer credits toward a degree or a certificate. Some of these programs are known to be predatory . The organizations we have listed above are Open Educational Resources - they are free and available to anyone who wants to work with them asynchronously, virtually, or in person. Using a version control system to host the analysis code and computational notebooks, and including these in your Methods section or Supplementary Materials, is an example of an Open Methodology? Success Yes! Using a VCS like GitHub or GitLab is a great step towards making your research more reproducible. Ways to improve your open methology can include documentation of your physical bench work, and even video recordings and step-by-step guides for every part of your project. You are asked to review a paper for an important journal in your field. The editor asks if you're willing to release your identity to the authors, thereby \"signing\" your review. Is this an example of \"Open Peer Review\"? Failure No Just because you've given your name to the author(s) of the manuscript, this does not make your review open. If the journal later publishes your review alongside the final manuscript, than you will have participated in an Open Review. You read a paper where the author(s) wrote their own code and licensed as \"Open Source\" software for a specific set of scientific tasks which you want to replicate. When you visit their personal website, you find the GitHub repository does not exist (because its now private). You contact the authors asking for access, but they refuse to share it 'due to competing researchers who are seeking to steal their intellectual property\". Is the software open source? Failure No Just because an author states they have given their software a permissive software license, does not make the software open source. Always make certain there is a LICENSE associated with any software you find on the internet. In order for the software to be open, it must follow the Open Source Initiative definition","title":"Introduction to Open Science"},{"location":"01_intro_open_sci/#introduction-to-open-science","text":"Learning Objectives After this lesson, you should be able to: Explain what Open Science is Explain the components of Open Science Describe the behaviors of Open Science Explain why Open Science matters in education, research, and society Understand the advantages and the challenges to Open Science Identify who the practitioners of Open Science are Understand the underlying Ethos of Open Science","title":"Introduction to Open Science"},{"location":"01_intro_open_sci/#what-is-open-science","text":"If you ask a dozen researchers this question, you will probably get just as many answers. This means that Open Science isn't necessarily a set of checkboxes you need to tick, but rather a holistic approach to doing science. Definitions \"Open Science is defined as an inclusive construct that combines various movements and practices aiming to make multilingual scientific knowledge openly available, accessible and reusable for everyone, to increase scientific collaborations and sharing of information for the benefits of science and society, and to open the processes of scientific knowledge creation, evaluation and communication to societal actors beyond the traditional scientific community.\" - UNESCO Definition UNESCO's Recommendation on Open Science \"Open Science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional...\" Wikipedia definition Open and Collaborative Science Network's Open Science Manifesto Six Pillars of Open Science Open Access Publications Open Data Open Educational Resources Open Methodology Open Peer Review Open Source Software Wait, how many pillars of Open Science Really Are There? The number can be from 4 to 8 Graphic by Foster Open Science flowchart LR id1([open science]) --> id3([publishing]) & id4([data]) & id5([open source software]) id3([publishing]) --> id41([access]) & id42([reviews]) & id43([methods]) & id44([educational resources]) id5([open source software]) --> id13([container registries]) & id10([services]) & id101([workflows]) & id12([version control systems]) id12([version control systems]) --> id101([workflows]) id13([container registries]) --> id101([workflows]) id14([public data registry]) --> id101([workflows]) id10([services]) --> id101([workflows]) id44([educational resources]) --> id21([university libraries]) id21([university libraries]) --> id101([workflows]) id22([federal data archives]) --> id101([workflows]) id4([data]) --> id21([university libraries]) & id22([federal data archives]) & id14([public data registries]) id101([workflows]) --> id15([on-premises]) & id16([commercial cloud]) & id17([public cloud]) Mermaid Diagram: Conceptual relationships of Open Science and cyberinfrastructure Awesome Lists of Open Science Awesome lists were started on GitHub by Sindre Sorhus and typically have a badge associated with them (There is even a Searchable Index of Awesome Lists) We have created our own Awesome Open Science List here which may be valuable to you.","title":"What is Open Science?"},{"location":"01_intro_open_sci/#open-access-publications","text":"Definitions \"Open access is a publishing model for scholarly communication that makes research information available to readers at no cost, as opposed to the traditional subscription model in which readers have access to scholarly information by paying a subscription (usually via libraries).\" -- OpenAccess.nl Pre-print Services ASAPbio Pre-Print Server List - ASAPbio is a scientist-driven non-profit promoting transparency and innovation comprehensive list of pre-print servers inthe field of life science communication. ESSOar - Earth and Space Science Open Archive hosted by the American Geophysical Union. Peer Community In (PCI) a free recommendation process of scientific preprints based on peer reviews OSF.io Preprints are partnered with numerous projects under the \"-rXivs\" The rXivs AfricArXiv AgrirXiv Arabixiv arXiv - is a free distribution service and an open-access archive for 2,086,431 scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics. BioHackrXiv BioRxiv - is an open access preprint repository for the biological sciences. BodorXiv EarthArXiv - is an open access preprint repository for the Earth sciences. EcsArXiv - a free preprint service for electrochemistry and solid state science and technology EdArXiv - for the education research community EngrXiv for the engineering community EvoEcoRxiv - is an open acccess preprint repository for Evolutionary and Ecological sciences. MediArXiv for Media, Film, & Communication Studies MedRxiv - is an open access preprint repository for Medical sciences. PaleorXiv - is an open access preprint repository for Paleo Sciences PsyrXiv - is an open access preprint repository for Psychological sciences. SocArXiv - is an open access preprint repository for Social sciences. SportrXiv - is an open access preprint for Sports sciences. ThesisCommons - open Theses Financial Support for Open Access Publishing Fees There are mechanisms for helping to pay for the additional costs of publishing research as open access: SciDevNet Health InterNetwork Access to Research Initiative (HINARI) Some institutions offer support for managing publishing costs (check to see if your institution has such support): University of Arizona Open Access Investment Fund Colorado University at Boulder Open Access Fund Max Planck Digital Library - German authors can have OA fees in Springer Nature research journals paid for. Bibsam Consortium - Swedish authors can have OA fees in Springer Nature research journals paid for. Open Access Publishing Major publishers have provided access points for publishing your work AAAS Nature American Geophysical Union Commonwealth Scientific and Industrial Research Organisation (CSIRO) Open Research Europe","title":" Open Access Publications"},{"location":"01_intro_open_sci/#open-data","text":"Open Data are a critical aspect of open science. There are three key attributes of Open Data: Availability and accessibility Reusability Inclusivity Definitions \u201cOpen data and content can be freely used, modified, and shared by anyone for any purpose\u201d - The Open Definition \"Open data is data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and sharealike.\" - Open Data Handbook Wikipedia definition DIKW Pyramid Data are the basis of our understanding the natural world. The Data-Information-Knowledge-Wisdom (DIKW) pyramid describes for us how data are refined into information and knowledge. FAIR & CARE Principles Wilkinson et al. (2016) established the guidelines to improve the Findability, Accessibility, Interoperability, and Reuse (FAIR) of digital assets for research. Go-FAIR website Carroll et al. (2020) established the CARE Principles for Indigenous Data Governance. full document Indigenous Data Sovereignty Networks LOD Cloud The Linked Open Data Cloud shows how data are linked to one another forming the basis of the semantic web .","title":" Open Data"},{"location":"01_intro_open_sci/#open-educational-resources","text":"Definitions \"Open Educational Resources (OER) are learning, teaching and research materials in any format and medium that reside in the public domain or are under copyright that have been released under an open license, that permit no-cost access, re-use, re-purpose, adaptation and redistribution by others.\" - UNESCO Wikipedia definition Digital Literacy Organizations The Carpentries - teaches foundational coding and data science skills to researchers worldwide EdX - Massively Online Online Courses (not all open) hosted through University of California Berkeley EveryoneOn - mission is to unlock opportunity by connecting families in underserved communities to affordable internet service and computers, and delivering digital skills trainings ConnectHomeUSA - is a movement to bridge the digital divide for HUD-assisted housing residents in the United States under the leadership of national nonprofit EveryoneOn Global Digital Literacy Council - has dedicated more than 15 years of hard work to the creation and maintenance of worldwide standards in digital literacy IndigiData - training and engaging tribal undergraduate and graduate students in informatics National Digital Equity Center a 501c3 non-profit, is a nationally recognized organization with a mission to close the digital divide across the United States National Digital Inclusion Allaince - advances digital equity by supporting community programs and equipping policymakers to act Net Literacy Open Educational Resources Commons Project Pythia is the education working group for Pangeo and is an educational resource for the entire geoscience community Research Bazaar - is a worldwide festival promoting the digital literacy emerging at the centre of modern research TechBoomers - is an education and discovery website that provides free tutorials of popular websites and Internet-based services in a manner that is accessible to older adults and other digital technology newcomers Educational Materials Teach Together by Greg Wilson DigitalLearn","title":" Open Educational Resources"},{"location":"01_intro_open_sci/#open-methodology","text":"The use of version control systems like GitHub and GitLab present one of the foremost platforms for sharing open methods for digital research. Definitions \"An open methodology is simply one which has been described in sufficient detail to allow other researchers to repeat the work and apply it elsewhere.\" - Watson (2015) \"Open Methodology refers to opening up methods that are used by researchers to achieve scientific results and making them publicly available.\" - Open Science Network Austria Protocols and Bench Techniques BioProtocol Current Protocols Gold Biotechnology Protocol list JoVE - Journal of Visualized Experiments Nature Protocols OpenWetWare Protocol Exchange Protocols Online Protocols SciGene Springer Nature Experiments","title":" Open Methodology"},{"location":"01_intro_open_sci/#open-peer-review","text":"Pros and Cons of Open Peer Review Definitions Ross-Hellauer et al. (2017) ask What is Open Peer Review? and state that there is no single agreed upon definition Wikipedia's definition Open Peer Review Resources F1000Research the first open research publishing platform. Offering open peer review rapid publication PREreview provides a space for open peer reviews, targeted toward early career researchers. ASAPbio Accelerating Science and Publication in Biology, an open peer review source for biologists and life scientists. PubPeer platform for post-publication of peer reviews. Sciety platform for evaluating preprints.","title":" Open Peer Review"},{"location":"01_intro_open_sci/#open-source-software","text":"Definitions \"Open source software is code that is designed to be publicly accessible\u2014anyone can see, modify, and distribute the code as they see fit. Open source software is developed in a decentralized and collaborative way, relying on peer review and community production.\" - Red Hat Open Source Initiative definition Wikipedia definition Awesome list","title":" Open Source Software"},{"location":"01_intro_open_sci/#breakout-discussion","text":"As you already know, being a scientist requires you to wear many hats, and trying to do Open Science is no different. Bernery et al. (2022) Figure 2: The positive aspects of doing a PhD. As we mentioned, Open Science is not a set of boxes you need to check off to be \"Certified Open\", but an intersecting set of philosophies and approaches, all of which occur on some type of spectrum. To get a feel for how Open Science can be multifaceted and different for each researcher, we will do a short breakout group session to discuss what Open Science means to you. What does Open Science mean to you? Which of the pillars of Open Science is nearest to your own heart? Open Access Publications Open Data Open Educational Resources Open Methodology Open Peer Review Open Source Software Are any of the pillars more important than the others? Are there any pillars not identified that you think should be considered?","title":"Breakout Discussion"},{"location":"01_intro_open_sci/#components-of-open-science","text":"One of the most fundamental, and certainly the most publicized component of Open Science is the accessibility of data. This makes sense - without access to your data, nothing else about your science can be all that open. While we will devote an entire 10 weeks of this course to data, opening up your data is only one piece of the puzzle. Gallagher et al (2020) Box 2: six core principles of Open Science. This figure demonstrates the multiple intersecting pieces of Open Science, which go beyond simply making data accessible. While we focus primarily on Open Data, Open Source, and Open Methodology in FOSS, it's worth considering how other parts of the scientific process might be opened up more broadly. Another component which sort of covers all of the pictured components, or at least links a lot of them together, might be referred to as Open Process. In response to the Reproducibility Crisis, many researchers, particularly in fields like psychology, have begun to advocate for preregistration of studies. This involves writing out and publishing your entire research plan, from data collection to analysis and publication, for the sake of avoiding practices like p-hacking or HARKing . What preregistration also does is make the process of your work more open, including many of the small decisions and tweaks you make to a project that probably wouldn't make it into a manuscript. To learn more about preregistration, you can check out the Open Science Foundation , a project that provides a preregistration platform and other Open Science tools. As mentioned above, it is worthwhile to think about Open Science not as a set of checkboxes, but rather a holistic approach to doing science. In that spirit, it can also be useful to think about Open Science as a spectrum, from less to more open. While you might not achieve some platonic ideal of openness for a variety of reasons, you can still make great progress in moving your science towards the Open end of the spectrum. In reality, a large scientific project probably consists of multiple spectra; you can move your data towards the open end of the spectrum while your software remains less open, and vice versa. All this is to say that doing Open Science is not a static set of goals you must achieve, it is a process that grows and changes with your science itself. One of the biggest challenges of doing science is that you might have to wear many different hats: domain expert, lab manager, statistician, teacher, mentor, grant writer, manuscript author, public speaker... the list goes on. Doing Open Science is no different, but the list of skills may be even greater, since the goal is now to openly communicate each step of the process to a broader audience. This also makes teaching Open Science quite challenging- we will cover topics ranging from \"soft skills\" like project management and internal communications to more technical skills like software management and containers. We could probably teach this whole workshop on each single topic, but we clearly don't have the time to do that. Instead, we will focus on a higher-level look at the landscape of Open Science and introduce you to a wide variety of skills and concepts with the idea that you can go on to find ways to implement them in your own work. What characteristics might a paper, project, lab group require to qualify as doing Open Science What are some limitations to you, your lab group, or your domain?","title":"Components of Open Science"},{"location":"01_intro_open_sci/#why-do-open-science","text":"There are many reasons to do Open Science, and presumably one or more of them brought you to this workshop. Whether you feel an ethical obligation, want to improve the quality of your work, or want to look better to funding agencies, many of the same approaches to Open Science apply. A paper from Bartling & Friesike (2014) posits that there are 5 main schools of thought in Open Science, which represent 5 underlying motivations: Democratic school : primarily concerned with making scholarly work freely available to everyone Pragmatic school : primarily concerned with improving the quality of scholarly work by fostering collaboration and improving critiques Infrastructure school : primarily focused on the platforms, tools, and services necessary to conduct efficient research, collaboration, and communication Public school : primarily concerned with societal impact of scholarly work, focusing on engagement with broader public via citizen science, understandable scientific communication, and less formal communication Measurement school : primarily concerned with the existing focus on journal publications as a means of measuring scholarly output, and focused on developing alternative measurements of scientific impact In Bartling & Friesike (2014) Open Science: One Term, Five Schools of Thought While many researchers may be motivated by one or more of these aspects, we will not necessarily focus on any of them in particular. If anything, FOSS may be slightly more in the Infrastructure school, because we aim to give you the tools to do Open Science based on your own underlying motivations. Let's break out into groups again to discuss some of our motivations for doing Open Science. What motivates you to do Open Science? Do you feel that you fall into a particular \"school\"? If so, which one, and why? Are there any motivating factors for doing Open Science that don't fit into this framework?","title":"WHY do Open Science?"},{"location":"01_intro_open_sci/#ethos-of-open-science","text":"Doing Open Science requires us to understand the ethics of why working with data which do not belong to us is privileged. We must also anticipate how these could be re-used in ways contrary to the interests of humanity . Ensure the use of Institutional Review Boards (IRB) or your local ethical committee. Areas to consider: Source: UK Statistics Authority Geolocation (survey, land ownership, parcel data), see UK Statistics Authority Ethical Considerations Personal identification information US Personal Identifiable Information (PII) , General Data Protection Regulation (GDPR) Health information US HIPAA , EU GDPR Protected and Endangered Species ( US Endangered Species Act ) Indigenous data sovereignty: CARE Principles for Indigenous Data Governance , Global Indigenous Data Alliance (GIDA) , First Nations OCAP\u00ae (Ownership Control Access and Possession) , Circumpolar Inuit Protocols for Equitable and Ethical Engagement Artificial intelligence/machine learning Assessment List Trustworthy AI (ALTAI) from the European AI Alliance \"Nothing about us, without us\" Funnel et al. (2019) For more information (training): Ethics and Data Access (General Information with BioMedical and Life Sciences Data) includes a legal and ethical checklist lesson for researchers around \"FAIR Plus\".","title":"Ethos of Open Science"},{"location":"01_intro_open_sci/#recommended-open-science-communities","text":"Open Scholarship Grassroots Community Networks International Open Science Networks Center for Scientific Collaboration and Community Engagement (CSCCE) Center for Open Science (COS) Eclipse Science Working Group eLife NumFocus Open Access Working Group Open Research Funders Group Open Science Foundation Open Science Network pyOpenSci R OpenSci Research Data Alliance (RDA) The Turing Way UNESCO Global Open Science Partnership World Wide Web Consortium (W3C) US-based Open Science Networks CI Compass - provides expertise and active support to cyberinfrastructure practitioners at USA NSF Major Facilities in order to accelerate the data lifecycle and ensure the integrity and effectiveness of the cyberinfrastructure upon which research and discovery depend. Earth Science Information Partners (ESIP) Federation - is a 501\u00a9(3) nonprofit supported by NASA, NOAA, USGS and 130+ member organizations. Internet2 - is a community providing cloud solutions, research support, and services tailored for Research and Education. Minority Serving Cyberinfrastructure Consortium (MS-CC) envisions a transformational partnership to promote advanced cyberinfrastructure (CI) capabilities on the campuses of Historically Black Colleges and Universities (HBCUs), Hispanic-Serving Institutions (HSIs), Tribal Colleges and Universities (TCUs), and other Minority Serving Institutions (MSIs). NASA Transform to Open Science (TOPS) - coordinates efforts designed to rapidly transform agencies, organizations, and communities for Earth Science OpenScapes - is an approach for doing better science for future us The Quilt - non-profit regional research and education networks collaborate to develop, deploy and operate advanced cyberinfrastructure that enables innovation in research and education. Oceania Open Science Networks New Zealand Open Research Network - New Zealand Open Research Network (NZORN) is a collection of researchers and research-associated workers in New Zealand. Australia & New Zealand Open Research Network - ANZORN is a network of local networks distributed without Australia and New Zealand.","title":"Recommended Open Science Communities"},{"location":"01_intro_open_sci/#self-assessment","text":"True or False: All research papers published in the top journals, like Science and Nature, are always Open Access? Failure False Major Research journals like Science and Nature have an \"Open Access\" option when a manuscript is accepted, but they charge an extra fee to the authors to make those papers Open Access. These high page costs are exclusionary to the majority of global scientists who cannot afford to front these costs out of pocket. This will soon change, at least in the United States. The Executive Branch of the federal government recently mandated that future federally funded research be made Open Access after 2026. True or False: an article states all of the research data used in the experiments \"are available upon request from the corresponding author(s),\" meaning the data are \"Open\" Failure False In order for research to be open, the data need to be freely available from a digital repository, like Data Dryad , Zenodo.org , or CyVerse . Data that are 'available upon request' do not meet the FAIR data principles. True or False: Online Universities and Data Science Boot Camps like UArizona Online, Coursera, Udemy, etc. promote digital literacy and are Open Educational Resources? Failure False These examples are for-profit programs which teach data science and computer programming online. Some may be official through public or private universities and offer credits toward a degree or a certificate. Some of these programs are known to be predatory . The organizations we have listed above are Open Educational Resources - they are free and available to anyone who wants to work with them asynchronously, virtually, or in person. Using a version control system to host the analysis code and computational notebooks, and including these in your Methods section or Supplementary Materials, is an example of an Open Methodology? Success Yes! Using a VCS like GitHub or GitLab is a great step towards making your research more reproducible. Ways to improve your open methology can include documentation of your physical bench work, and even video recordings and step-by-step guides for every part of your project. You are asked to review a paper for an important journal in your field. The editor asks if you're willing to release your identity to the authors, thereby \"signing\" your review. Is this an example of \"Open Peer Review\"? Failure No Just because you've given your name to the author(s) of the manuscript, this does not make your review open. If the journal later publishes your review alongside the final manuscript, than you will have participated in an Open Review. You read a paper where the author(s) wrote their own code and licensed as \"Open Source\" software for a specific set of scientific tasks which you want to replicate. When you visit their personal website, you find the GitHub repository does not exist (because its now private). You contact the authors asking for access, but they refuse to share it 'due to competing researchers who are seeking to steal their intellectual property\". Is the software open source? Failure No Just because an author states they have given their software a permissive software license, does not make the software open source. Always make certain there is a LICENSE associated with any software you find on the internet. In order for the software to be open, it must follow the Open Source Initiative definition","title":"Self Assessment"},{"location":"02_project_management/","text":"Introduction to Project Management \u00b6 Learning Objectives After this lesson, you should be able to: Discuss different levels of project management Describe tools and approaches to managing collaborative projects Describe best practices for computational project organization Understand benefits of establishing project management practices from the start of a project until after it ends \" Project Management \" by itself may sound a bit vague and broad. Definition \"Project management is the use of specific knowledge, skills, tools and techniques to deliver something of value to people. The development of software for an improved business process, the construction of a building, the relief effort after a natural disaster, the expansion of sales into a new geographic market\u2014these are all examples of projects.\" - Project Management Institute Wikipedia definition Here we use the term in two different contexts. First, we'll go over the project management of scientific labs, groups, and projects, talking about things like governance, how to develop operations manuals, laying out roles and responsibilities, planning steps and the workflows which connect them. Next, we'll go over project management as \" research objects \": making sure your data, code, and documents are well-organized. These are crucial for future topics like version control and reproducibility. What is Project Management? \u00b6 This type of overall project management may be required for some grants, and while it may be tempting to put in the minimal effort on one of the many pieces of paperwork you're required to complete, this type of overall project planning can be very useful. The Turing Way offer a lesson on Project Design related to effective project planning and management. Project Governance \u00b6 Definitions Project Governance is the set of rules, procedures and policies that determine how projects are managed and overseen. \"The set of policies, regulations, functions, processes, and procedures and responsibilities that define the establishment, management and control of projects, programmes or portfolios.\" - APM (2012) , open.edu Wikipedia Definition No matter how small, i.e., even single person-run projects, a good Project Governance structure can help keep work on track and headed toward a timely finish. Establishing a project governance document at the onset of a project is a good way of setting boundaries, roles and responsibilities, pre-registration about what deliverables are expected, and what the consequences will be for breaking trust. Example Governance Documents Munoz-Torres et al. 2020 Research Collaborations \u00b6 Sahneh & Balk et al. (2020) Ten simple rules to cultivate transdisciplinary collaboration in data science, discuss the interactions amongst teams of diverse researchers. Sahneh & Balk et al. (2020) Fig 1. How the rules work together and intersect. There are multiple components in collaborations: person\u2013person interactions, person\u2013technology interactions, person\u2013data interactions, and data\u2013technology interactions. Synergy between these components results in a successful collaboration. Breakout Discussion \u00b6 Now we will do a breakout discussion section to talk about overall project management. What are some project management strategies that you have found to work well? What is an example of a poorly managed project you were involved in? What contributed to this feeling? Why do you think effective project management is important to Open Science? What are some limitations to you, your lab/group, or your domain? Team Roles and Responsibilities \u00b6 It can be easy for certain tasks to slip through the cracks. Established roles and responsibilities of teams can help ensure nobody gets saddled with too much work, and reduces chances of disputes among collaborators. Project Management Professional (PMP)\u00ae A Project Management Professional (PMP)\u00ae certification has been embraced globally as adding value to your professional resume. Academia has also embraced PMP certification as part of continuing education for academic staff and faculty. University of Arizona PMP prep Team roles and titles Again, The Turing Way provide an excellent set of examples of infrastructure job titles and roles on software driven projects: Community Manager - \"responsibilities include establishing engagement, organising community spaces and events, supporting people through inclusive practices, developing and maintaining resources, growing and evaluating use cases and collaborating with people involved in research and scientific communities.\" ( 1 , 2 ) This image was created by Scriberia for The Turing Way community and is used under a CC-BY 4.0 licence. Data Science Educator - \"... data science in education refers to the application of data science methods, while other times it refers to data science as a context for teaching and learning\" Rosenberg et al. (2020) , Estrellado et al. Data Scientist - a professional who uses analytical, statistical, and programming skills to collect, analyze, and describe data. Data Steward - \"... responsible for ensuring the quality and fitness for purpose of the organization's data assets, including the metadata for those data assets.\" - Wikipedia Developer Advocate - sometimes called platform evangelism , advocates represent the voice of the user (in the case of open science, the scientists) internally to the project team or company, and the voice of the project or company externally to the public. DevOps Engineer - a combinination of software development \"Dev\" and IT operations \"Ops\", responsibilities focus on \" continuous delivery \" and agile software development Research Application Manager (RAM) - in some ways a combination of Community Manager and Developer Advocate, Fig. 94 Research Application Managers work with the research team to embed outputs into user organisations. The Turing Way Community, & Scriberia. (2020, November). Illustrations from the Turing Way book dashes. Zenodo. http://doi.org/10.5281/zenodo.4323154 Research Software Engineer - those who regularly use expertise in programming to advance research - US Research Software Engineer (US-RSE) Association Open Source Research Software Maintainer \u00b6 Becoming an open source software maintainer is not to be taken likely. Image Credit: XKCD Dependency When you create a new software, library, or package, you are becoming its parent and guardian. What are Research Objects? \u00b6 Definition \"A workflow-centric research object bundles a workflow, the provenance of the results obtained by its enactment, other digital objects that are relevant for the experiment (papers, datasets, etc.), and annotations that semantically describe all these objects.\" - Corcho et al. 2012 \"... semantically rich aggregations of resources, that can possess some scientific intent or support some research objective.\" - Bechhofer et al. 2010 Wikipedia definition When we talk about project management in this section, we mean the way you organize data, code, images, documents, and documentation within a project. One way to think about this is in the context of \" research objects \" which condense into a single end point (think: a URL like a digital object identifier (DOI)) where others can come to reproduce your research. A conceptual workflow for developing research objects with cyberinfrastructure Research Object Services ResearchObject ROHub - Garcia-Silva et al. 2019 If you've ever had to navigate someone else's computer or a GitHub repository, you probably know that a poorly organized project can greatly reduce its accessibility. On the other hand, a well-organized project can: make your work more accessible to others help collaborators effectively contribute to your project ease the growing pains of a rapidly scaling project make life much easier for your future self It can be easy to overlook sound project management, opting for a \"just get it done ASAP\" approach to your work, but this almost always costs you more time in the end. The best time to introduce good project management is at the start of a project, and the second best time is right now. An hour spent reorganizing a project today may save you days of headaches later on. There is no single \"right way\" to organize and manage a project, and your specific needs may vary, but we will introduce a basic framework that can get you headed in the right direction. It will also help us introduce more advanced topics later on, as many of the skills and approaches we teach will be far more effective if the basic project organization is sound. Development Methology the \"leaps of faith\" required in Agile vs Waterfall. Image Credit: Wikimedia Commons CC BY 4.0 In software development, there are two common methologies which have similar applications to a research project. Agile Scrum - is one technique within Agile Kanban - is one technique within Agile Waterfall the effort distribtion of Agile vs Waterfall. Image Credit: Wikimedia Commons CC BY 4.0 LucidChart Blog: Agile vs Waterfall vs Kanban vs Scrum Ontology of Value: Agile vs Waterfall vs Kanban vs Scrum Hands on \u00b6 For the duration of the workshop, we will be working with a basic example project pipeline, going from raw data to a simple report. While this example project is vastly simplified from a real research project, the basic steps are still found in most projects. The skills you learn to improve this example project will translate to larger and more complex research projects. Your project will almost certainly differ from this general workflow in some way, but many of the basic steps will be similar: you collect some data, manipulate it, analyze it, make some outputs, and produce some reports. There may be many additional steps and sub-steps for each of these processes, but the key thing is how they relate to each other and how you manage the relationships and organization of various files. Data Organization \u00b6 Example data project organization from UArizona Libraries CookieCutter Templates Example project structure: . \u251c\u2500\u2500 AUTHORS.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin <- Your compiled model code can be stored here (not tracked by git) \u251c\u2500\u2500 config <- Configuration files, e.g., for doxygen or for your model if needed \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u251c\u2500\u2500 docs <- Documentation, e.g., doxygen or scientific papers (not tracked by git) \u251c\u2500\u2500 notebooks <- Ipython or R notebooks \u251c\u2500\u2500 reports <- For a manuscript source, e.g., LaTeX, Markdown, etc., or any project reports \u2502 \u2514\u2500\u2500 figures <- Figures for the manuscript or reports \u2514\u2500\u2500 src <- Source code for this project \u251c\u2500\u2500 data <- scripts and programs to process data \u251c\u2500\u2500 external <- Any external source code, e.g., pull other git projects, or external libraries \u251c\u2500\u2500 models <- Source code for your own model \u251c\u2500\u2500 tools <- Any helper scripts go here \u2514\u2500\u2500 visualization <- Scripts for visualisation of your results, e.g., matplotlib, ggplot2 related. Best Practices Projects should be self-contained this is probably the most important concept strictly necessary for version control use relative paths Use structure to organize files Don't underestimate complexity Keep raw data raw Treat generated output as disposable Avoid manual (point-and-click) steps as much as possible if necessary, record in detail should also be recorded in prior and subsequent steps Avoid spaces in file and folder names consider snake_case camelCase PascalCase kebab-case instead Describe structure in README The best time to organize is at the start, the 2 nd best is right now Reorganize if necessary, but don't overdo it Using same basic structure can help you navigate new/old projects Productivity Software CryptPad - online rich text pad. Draw.io - drawings and diagrams in browser. Excel - love it or hate it, many people still work in it or with .xlsx format files. Google Docs - is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google. HackMD - online markdown editor. JupyterBook - create documentation using Jupyter Notebooks and Markdown MkDocs - is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. LaTeX - is a high-quality typesetting system Overleaf - LaTeX online document sharing platform. ReadTheDocs - documentation using a variety of Markup langages Software Heritage - preserves software source code for present and future generations. Project Management Software OSF.io Examples Atlassian Confluence Jira Trello GitHub Issues Open Project ZenHub Key Concepts for Research Objects Project Critique Activity For this activity, the instructor will walk you through an example of a simple project workflow on their computer. Take notes on anything you notice that might be a hindrance to the openness or reproducibility of the project. We will then start breakout sessions for 5-10 minutes for group discussions on any issues you noticed and how you might improve on them. Create a working directory You might find a nice basic structure that works as a good starting place for many of your projects, or smaller components of big projects. Instead of having to repeat the process of making that directory structure, which could be tedious and introduce mistakes, you could write some code to do it for you. The following is a bash script that takes one argument, the name of the new project (with no spaces), and creates that project with a premade directory structure for you to put files into. #!/usr/bin/env bash # Run this script with the name of the new project as # an argument, like so: `bash make_project.sh my_project` # It will generate a project with the following structure: #. #|-- README.md #|-- data #| |-- cleaned #| `-- raw #|-- images #|-- reports #`-- scripts mkdir \"$1\" cd \"$1\" || exit echo \"# $1\" >> README.md mkdir data mkdir data/raw mkdir data/cleaned mkdir scripts mkdir images mkdir reports This approach to automating repetitive tasks is something we'll dig into even deeper in later lessons. Other Resources \u00b6 There are many other resources on more specific elements of project management. We'll link to some of them here. Using R Projects with RStudio: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects Using the R package here : https://github.com/jennybc/here_here and https://here.r-lib.org/ An even more compartmentalized approach to project management: https://hrdag.org/2016/06/14/the-task-is-a-quantum-of-workflow/ Self Assessment \u00b6 What are things that we do with data under the auspices of \"Data Management\"? Answers Data collection Data entry Data validation Data preservation Metadata What are common steps to cleaning data? Answers Validation Exploratory analysis/sleuthing Naming Structuring Consistency Research Objects must include all components of research: governance document, manuals, documentation, research papers, analysis code, data, software containers Answers While a Research Object (RO) may include the entire kitchen sink from a research project, it does NOT always contain all of these things. Fundamentally, a RO should contain enough information and detail to reproduce a scientific study from its linked or self-contained parts. Components like large datasets may not be a part of the RO, but the code or analysis scripts should have the ability to connect to or stream those data.","title":"Introduction to Project Management"},{"location":"02_project_management/#introduction-to-project-management","text":"Learning Objectives After this lesson, you should be able to: Discuss different levels of project management Describe tools and approaches to managing collaborative projects Describe best practices for computational project organization Understand benefits of establishing project management practices from the start of a project until after it ends \" Project Management \" by itself may sound a bit vague and broad. Definition \"Project management is the use of specific knowledge, skills, tools and techniques to deliver something of value to people. The development of software for an improved business process, the construction of a building, the relief effort after a natural disaster, the expansion of sales into a new geographic market\u2014these are all examples of projects.\" - Project Management Institute Wikipedia definition Here we use the term in two different contexts. First, we'll go over the project management of scientific labs, groups, and projects, talking about things like governance, how to develop operations manuals, laying out roles and responsibilities, planning steps and the workflows which connect them. Next, we'll go over project management as \" research objects \": making sure your data, code, and documents are well-organized. These are crucial for future topics like version control and reproducibility.","title":"Introduction to Project Management"},{"location":"02_project_management/#what-is-project-management","text":"This type of overall project management may be required for some grants, and while it may be tempting to put in the minimal effort on one of the many pieces of paperwork you're required to complete, this type of overall project planning can be very useful. The Turing Way offer a lesson on Project Design related to effective project planning and management.","title":"What is Project Management?"},{"location":"02_project_management/#project-governance","text":"Definitions Project Governance is the set of rules, procedures and policies that determine how projects are managed and overseen. \"The set of policies, regulations, functions, processes, and procedures and responsibilities that define the establishment, management and control of projects, programmes or portfolios.\" - APM (2012) , open.edu Wikipedia Definition No matter how small, i.e., even single person-run projects, a good Project Governance structure can help keep work on track and headed toward a timely finish. Establishing a project governance document at the onset of a project is a good way of setting boundaries, roles and responsibilities, pre-registration about what deliverables are expected, and what the consequences will be for breaking trust. Example Governance Documents Munoz-Torres et al. 2020","title":"Project Governance"},{"location":"02_project_management/#research-collaborations","text":"Sahneh & Balk et al. (2020) Ten simple rules to cultivate transdisciplinary collaboration in data science, discuss the interactions amongst teams of diverse researchers. Sahneh & Balk et al. (2020) Fig 1. How the rules work together and intersect. There are multiple components in collaborations: person\u2013person interactions, person\u2013technology interactions, person\u2013data interactions, and data\u2013technology interactions. Synergy between these components results in a successful collaboration.","title":"Research Collaborations"},{"location":"02_project_management/#breakout-discussion","text":"Now we will do a breakout discussion section to talk about overall project management. What are some project management strategies that you have found to work well? What is an example of a poorly managed project you were involved in? What contributed to this feeling? Why do you think effective project management is important to Open Science? What are some limitations to you, your lab/group, or your domain?","title":"Breakout Discussion"},{"location":"02_project_management/#team-roles-and-responsibilities","text":"It can be easy for certain tasks to slip through the cracks. Established roles and responsibilities of teams can help ensure nobody gets saddled with too much work, and reduces chances of disputes among collaborators. Project Management Professional (PMP)\u00ae A Project Management Professional (PMP)\u00ae certification has been embraced globally as adding value to your professional resume. Academia has also embraced PMP certification as part of continuing education for academic staff and faculty. University of Arizona PMP prep Team roles and titles Again, The Turing Way provide an excellent set of examples of infrastructure job titles and roles on software driven projects: Community Manager - \"responsibilities include establishing engagement, organising community spaces and events, supporting people through inclusive practices, developing and maintaining resources, growing and evaluating use cases and collaborating with people involved in research and scientific communities.\" ( 1 , 2 ) This image was created by Scriberia for The Turing Way community and is used under a CC-BY 4.0 licence. Data Science Educator - \"... data science in education refers to the application of data science methods, while other times it refers to data science as a context for teaching and learning\" Rosenberg et al. (2020) , Estrellado et al. Data Scientist - a professional who uses analytical, statistical, and programming skills to collect, analyze, and describe data. Data Steward - \"... responsible for ensuring the quality and fitness for purpose of the organization's data assets, including the metadata for those data assets.\" - Wikipedia Developer Advocate - sometimes called platform evangelism , advocates represent the voice of the user (in the case of open science, the scientists) internally to the project team or company, and the voice of the project or company externally to the public. DevOps Engineer - a combinination of software development \"Dev\" and IT operations \"Ops\", responsibilities focus on \" continuous delivery \" and agile software development Research Application Manager (RAM) - in some ways a combination of Community Manager and Developer Advocate, Fig. 94 Research Application Managers work with the research team to embed outputs into user organisations. The Turing Way Community, & Scriberia. (2020, November). Illustrations from the Turing Way book dashes. Zenodo. http://doi.org/10.5281/zenodo.4323154 Research Software Engineer - those who regularly use expertise in programming to advance research - US Research Software Engineer (US-RSE) Association","title":"Team Roles and Responsibilities"},{"location":"02_project_management/#open-source-research-software-maintainer","text":"Becoming an open source software maintainer is not to be taken likely. Image Credit: XKCD Dependency When you create a new software, library, or package, you are becoming its parent and guardian.","title":"Open Source Research Software Maintainer"},{"location":"02_project_management/#what-are-research-objects","text":"Definition \"A workflow-centric research object bundles a workflow, the provenance of the results obtained by its enactment, other digital objects that are relevant for the experiment (papers, datasets, etc.), and annotations that semantically describe all these objects.\" - Corcho et al. 2012 \"... semantically rich aggregations of resources, that can possess some scientific intent or support some research objective.\" - Bechhofer et al. 2010 Wikipedia definition When we talk about project management in this section, we mean the way you organize data, code, images, documents, and documentation within a project. One way to think about this is in the context of \" research objects \" which condense into a single end point (think: a URL like a digital object identifier (DOI)) where others can come to reproduce your research. A conceptual workflow for developing research objects with cyberinfrastructure Research Object Services ResearchObject ROHub - Garcia-Silva et al. 2019 If you've ever had to navigate someone else's computer or a GitHub repository, you probably know that a poorly organized project can greatly reduce its accessibility. On the other hand, a well-organized project can: make your work more accessible to others help collaborators effectively contribute to your project ease the growing pains of a rapidly scaling project make life much easier for your future self It can be easy to overlook sound project management, opting for a \"just get it done ASAP\" approach to your work, but this almost always costs you more time in the end. The best time to introduce good project management is at the start of a project, and the second best time is right now. An hour spent reorganizing a project today may save you days of headaches later on. There is no single \"right way\" to organize and manage a project, and your specific needs may vary, but we will introduce a basic framework that can get you headed in the right direction. It will also help us introduce more advanced topics later on, as many of the skills and approaches we teach will be far more effective if the basic project organization is sound. Development Methology the \"leaps of faith\" required in Agile vs Waterfall. Image Credit: Wikimedia Commons CC BY 4.0 In software development, there are two common methologies which have similar applications to a research project. Agile Scrum - is one technique within Agile Kanban - is one technique within Agile Waterfall the effort distribtion of Agile vs Waterfall. Image Credit: Wikimedia Commons CC BY 4.0 LucidChart Blog: Agile vs Waterfall vs Kanban vs Scrum Ontology of Value: Agile vs Waterfall vs Kanban vs Scrum","title":"What are Research Objects?"},{"location":"02_project_management/#hands-on","text":"For the duration of the workshop, we will be working with a basic example project pipeline, going from raw data to a simple report. While this example project is vastly simplified from a real research project, the basic steps are still found in most projects. The skills you learn to improve this example project will translate to larger and more complex research projects. Your project will almost certainly differ from this general workflow in some way, but many of the basic steps will be similar: you collect some data, manipulate it, analyze it, make some outputs, and produce some reports. There may be many additional steps and sub-steps for each of these processes, but the key thing is how they relate to each other and how you manage the relationships and organization of various files.","title":"Hands on"},{"location":"02_project_management/#data-organization","text":"Example data project organization from UArizona Libraries CookieCutter Templates Example project structure: . \u251c\u2500\u2500 AUTHORS.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 bin <- Your compiled model code can be stored here (not tracked by git) \u251c\u2500\u2500 config <- Configuration files, e.g., for doxygen or for your model if needed \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u251c\u2500\u2500 docs <- Documentation, e.g., doxygen or scientific papers (not tracked by git) \u251c\u2500\u2500 notebooks <- Ipython or R notebooks \u251c\u2500\u2500 reports <- For a manuscript source, e.g., LaTeX, Markdown, etc., or any project reports \u2502 \u2514\u2500\u2500 figures <- Figures for the manuscript or reports \u2514\u2500\u2500 src <- Source code for this project \u251c\u2500\u2500 data <- scripts and programs to process data \u251c\u2500\u2500 external <- Any external source code, e.g., pull other git projects, or external libraries \u251c\u2500\u2500 models <- Source code for your own model \u251c\u2500\u2500 tools <- Any helper scripts go here \u2514\u2500\u2500 visualization <- Scripts for visualisation of your results, e.g., matplotlib, ggplot2 related. Best Practices Projects should be self-contained this is probably the most important concept strictly necessary for version control use relative paths Use structure to organize files Don't underestimate complexity Keep raw data raw Treat generated output as disposable Avoid manual (point-and-click) steps as much as possible if necessary, record in detail should also be recorded in prior and subsequent steps Avoid spaces in file and folder names consider snake_case camelCase PascalCase kebab-case instead Describe structure in README The best time to organize is at the start, the 2 nd best is right now Reorganize if necessary, but don't overdo it Using same basic structure can help you navigate new/old projects Productivity Software CryptPad - online rich text pad. Draw.io - drawings and diagrams in browser. Excel - love it or hate it, many people still work in it or with .xlsx format files. Google Docs - is an online word processor included as part of the free, web-based Google Docs Editors suite offered by Google. HackMD - online markdown editor. JupyterBook - create documentation using Jupyter Notebooks and Markdown MkDocs - is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. LaTeX - is a high-quality typesetting system Overleaf - LaTeX online document sharing platform. ReadTheDocs - documentation using a variety of Markup langages Software Heritage - preserves software source code for present and future generations. Project Management Software OSF.io Examples Atlassian Confluence Jira Trello GitHub Issues Open Project ZenHub Key Concepts for Research Objects Project Critique Activity For this activity, the instructor will walk you through an example of a simple project workflow on their computer. Take notes on anything you notice that might be a hindrance to the openness or reproducibility of the project. We will then start breakout sessions for 5-10 minutes for group discussions on any issues you noticed and how you might improve on them. Create a working directory You might find a nice basic structure that works as a good starting place for many of your projects, or smaller components of big projects. Instead of having to repeat the process of making that directory structure, which could be tedious and introduce mistakes, you could write some code to do it for you. The following is a bash script that takes one argument, the name of the new project (with no spaces), and creates that project with a premade directory structure for you to put files into. #!/usr/bin/env bash # Run this script with the name of the new project as # an argument, like so: `bash make_project.sh my_project` # It will generate a project with the following structure: #. #|-- README.md #|-- data #| |-- cleaned #| `-- raw #|-- images #|-- reports #`-- scripts mkdir \"$1\" cd \"$1\" || exit echo \"# $1\" >> README.md mkdir data mkdir data/raw mkdir data/cleaned mkdir scripts mkdir images mkdir reports This approach to automating repetitive tasks is something we'll dig into even deeper in later lessons.","title":"Data Organization"},{"location":"02_project_management/#other-resources","text":"There are many other resources on more specific elements of project management. We'll link to some of them here. Using R Projects with RStudio: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects Using the R package here : https://github.com/jennybc/here_here and https://here.r-lib.org/ An even more compartmentalized approach to project management: https://hrdag.org/2016/06/14/the-task-is-a-quantum-of-workflow/","title":"Other Resources"},{"location":"02_project_management/#self-assessment","text":"What are things that we do with data under the auspices of \"Data Management\"? Answers Data collection Data entry Data validation Data preservation Metadata What are common steps to cleaning data? Answers Validation Exploratory analysis/sleuthing Naming Structuring Consistency Research Objects must include all components of research: governance document, manuals, documentation, research papers, analysis code, data, software containers Answers While a Research Object (RO) may include the entire kitchen sink from a research project, it does NOT always contain all of these things. Fundamentally, a RO should contain enough information and detail to reproduce a scientific study from its linked or self-contained parts. Components like large datasets may not be a part of the RO, but the code or analysis scripts should have the ability to connect to or stream those data.","title":"Self Assessment"},{"location":"03_managing_data/","text":"Managing Data \u00b6 Learning Objectives After this lesson, you should be able to: Recognize data as the foundation of open science and be able to describe the \"life cycle of data\" Use self-assessments to evaluate your current data management practices Cite tools and resources to improve your data management practices Know the biggest challenge to effective data management Why should you care about data management? \u00b6 Most scientific work centers on generating new data or working with existing data, which means researchers spend a lot of time dealing with it. Ensuring that data are effectively organized, shared, and preserved is critical to making your science impactful, efficient, and open. How would you answer? If you give your data to a colleague who has not been involved with your project, would they be able to make sense of it? Would they be able to use it properly? If you come back to your own data in five years, will you be able to make sense of it? Will you be able to use it properly? When you are ready to publish a paper, is it easy to find all the correct versions of all the data you used and present them in a comprehensible manner? Data management skills produce self-describing datasets that: Make life much easier for you and your collaborators Benefit the scientific research community by allowing others to reuse your data Are required by most funders and many journals Recent Dear Colleague letter from NSF NSF proposal preparation guidelines Data Self-assessment \u00b6 Part I: Basic questions Here are some questions about how you manage and work with data. We will complete some more formal assessments later, but for now let's see where you are. Activity In small groups, discuss the following questions. You will be provided with a space for documenting our shared answers. 1. What are the two or three data types that you most frequently work with? - Think about the sources (observational, experimental, simulated, compiled/derived) - Also consider the formats (tabular, sequence, database, image, etc.) 2. What is the scale of your data? Tip We often talk about the scale of data using the \"Three V's\" : - Volume: Size of the data (MBs, GBs, TBs); can also include how many files (e.g dozens of big files, or millions of small ones) - Velocity: How quickly are these data produced and analyzed? A lot coming in a single batch infrequently, or, a constant small amount of data that must be rapidly analyzed? - Variety: How many different data types (raw files? databases?) A fourth V (Veracity) captures the need to make decisions about data processing (i.e., separating low- and high-quality data) What is your strategy for storing and backing up your data? What is your strategy for verifying the integrity of your data? (i.e. verifying that your data has not be altered) What is your strategy for searching your data? What is your strategy for sharing (and getting credit for) your data? (i.e. How will do you share with your community/clients? How is that sharing documented? How do you evaluate the impact of data shared? ) Data Management Basics \u00b6 Let's learn a little more about data so that we can evaluate your self-assessment responses. Data Types \u00b6 Different types of data require different management practices. What are some data types and sources you might use in your work? (Adapted from DMP Tool Data management general guidance ) Data Types Text: field or laboratory notes, survey responses Numeric: tables, counts, measurements Audiovisual: images, sound recordings, video Models, computer code Discipline-specific: FASTA in biology, FITS in astronomy, CIF in chemistry Instrument-specific: equipment outputs Data Sources Observational Captured in real-time, typically outside the lab Usually irreplaceable and therefore the most important to safeguard Examples: Sensor readings, telemetry, survey results, images Experimental Typically generated in the lab or under controlled conditions Often reproducible, but can be expensive or time-consuming Examples: gene sequences, chromatograms, magnetic field readings Simulation Machine generated from test models Likely to be reproducible if the model and inputs are preserved Examples: climate models, economic models Derived / Compiled Generated from existing datasets Reproducible, but can be very expensive and time-consuming Examples: text and data mining, compiled database, 3D models Tip The Data Life Cycle Data management is the set of practices that allow researchers to effectively and efficiently handle data throughout the data life cycle. Although typically shown as a circle (below) the actually life cycle of any data item may follow a different path, with branches and internal loops. Being aware of your data's future helps you plan how to best manage them. Image from Strasser et al . Best practices for the data life cycle \u00b6 The most important thing to remember about data management is that you are not alone. There are data management experts within your discipline, many of them not farther away than your university library system. In addition, there are many organizations (including many cited below) which work to solve. Warning The biggest challenge to effective data management The biggest challenge to data management making it an afterthought. Unfortunately, poor data management doesn't have a high upfront cost. You can do substantial work before realizing you are in trouble. Like a swimmer in rip current, by the time you realize you are in trouble, you may already be close to drowning. The solution? Make data management the first thing you consider when starting a research project. It also needs to be a policy you institute right away for your research group. Here are some excellent steps to consider. The summary below is adapted from the excellent DataONE best practices primer . Plan Describe the data that will be compiled, and how the data will be managed and made accessible throughout its lifetime A good plan considers each of the stages below Collect Have a plan for data organization in place before collecting data Collect and store observation metadata at the same time you collect the metadata Take advantage of machine generated metadata Assure Record any conditions during collection that might affect the quality of the data Distinguish estimated values from measured values Double check any data entered by hand Perform statistical and graphical summaries (e.g., max/min, average, range) to check for questionable or impossible values. Mark data quality, outliers, missing values, etc. Describe: Comprehensive data documentation (i.e. metadata) is the key to future understanding of data. Without a thorough description of the context of the data, the context in which they were collected, the measurements that were made, and the quality of the data, it is unlikely that the data can be easily discovered, understood, or effectively used. Organize your data for publication. Before you can describe your data, you must decide how to organize them. This should be planned before hand, so that data organization is a minimal task at the time of publication. Thoroughly describe the dataset (e.g., name of dataset, list of files, date(s) created or modified, related datasets) including the people and organizations involved in data collection (e.g., authors, affiliations, sponsor). Also include: An ORCID (obtain one if you don't have one). The scientific context (reason for collecting the data, how they were collected, equipment and software used to generate the data, conditions during data collection, spatial and temporal resolution) The data themselves How each measurement was produced Units Format Quality assurance activities Precision, accuracy, and uncertainty Metadata standards and ontologies are invaluable for supporting data reuse. Metadata standards tell you: Which metadata attributes to include How to format your metadata What values are allowable for different attributes Some metadata standards you may want to consider: DataCite for publishing data Dublin Core for sharing data on the web MIxS Minimum Information for any (x) sequence OGC standards for geospatial data Ontologies provide standardization for metadata values: Example: Environment Ontology terms for the MIxS standards Example: Plant Ontology for plant tissue types or development stages FAIRSharing.org lists standards and ontologies for life sciences. The CyVerse Data Commons supports good data description through: Metadata templates Bulk metadata upload Automatic collection of analysis parameters, inputs, and outputs in the DE. Preserve In general, data must be preserved in an appropriate long-term archive (i.e. data center). Here are some examples: Sequence data should go to a national repository, frequently NCBI Identify data with value - it may not be necessary to preserve all data from a project The CyVerse Data Commons provides a place to publish and preserve data that was generated on or can be used in CyVerse, where no other repository exists. See lists of repositories at FAIRSharing.org See lists of repositories at Data Dryad Github repos can get DOIs through Zenodo Be aware of licensing and other intellectual property issues Repositories will require some kind of license, often the least restrictive (see for example Creative Commons ) Repositories are unlikely to enforce reuse restrictions, even if you apply them. Discover Good metadata allows you to discover your own data! Databases, repositories, and search indices provide ways to discover relevant data for reuse Google dataset search DataOne FAIRSharing.org Integrate Data integration is a lot of work Standards and ontologies are key to future data integration Know the data before you integrate them Don't trust that two columns with the same header are the same data Properly cite the data you reuse! Use DOIs ( Digital Object Identifiers ) wherever possible Analyze Follow open science principles for reproducible analyses (CyVerse, RStudio, notebooks, IDEs) State your hypotheses and analysis workflow before collecting data. Tools like Open Science Framework (OSF) allow you to make this public. Record all software, parameters, inputs, etc. References and Resources \u00b6 DataOne best practices Center for Open Science FAIR Data \u00b6 Learning Objectives Recall the meaning of FAIR Understand why FAIR is a collection of principles (rather than rules) Use self-assessments to evaluate the FAIRness of your data FAIR Principles \u00b6 In 2016, the FAIR Guiding Principles for scientific data management and stewardship were published in Scientific Data. Read it. Findable F1. (meta)data are assigned a globally unique and persistent identifier F2. data are described with rich metadata (defined by R1 below) F3. metadata clearly and explicitly include the identifier of the data it describes F4. (meta)data are registered or indexed in a searchable resource Accessible A1. (meta)data are retrievable by their identifier using a standardized communications protocol A1.1 the protocol is open, free, and universally implementable A1.2 the protocol allows for an authentication and authorization procedure, where necessary A2. metadata are accessible, even when the data are no longer available Interoperable I1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation. I2. (meta)data use vocabularies that follow FAIR principles I3. (meta)data include qualified references to other (meta)data Reusable R1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standard Tip Open vs. Public vs. FAIR: FAIR does not demand that data be open: See one definition of open: http://opendefinition.org/ Tip Why Principles? FAIR is a collection of principles. Ultimately, different communities within different scientific disciplines must work to interpret and implement these principles. Because technologies change quickly, focusing on the desired end result allows FAIR to be applied to a variety of situations now and in the foreseeable future. CARE Principles \u00b6 The CARE Principles for Indigenous Data Governance were drafted at the International Data Week and Research Data Alliance Plenary co-hosted event \"Indigenous Data Sovereignty Principles for the Governance of Indigenous Data Workshop,\" 8 November 2018, Gaborone, Botswana. Collective Benefit C1. For inclusive development and innovation C2. For improved governance and citizen engagement C3. For equitable outcomes Authority to Control A1. Recognizing rights and interests A2. Data for governance A3. Governance of data Responsibility R1. For positive relationships R2. For expanding capability and capacity R3. For Indigenous languages and worldviews Ethics E1. For minimizing harm and maximizing benefit E2. For justice E3. For future use FAIR - TLC \u00b6 Traceable, Licensed, and Connected The need for metrics: https://zenodo.org/record/203295#.XkrzTxNKjzI How to get to FAIR? \u00b6 This is a question that only you can answer, that is because it depends on (among other things) Your scientific discipline: Your datatypes and existing standards for what constitutes acceptable data management will vary. The extent to which your scientific community has implemented FAIR: Some disciplines have significant guidelines on FAIR, while others have not addressed the subject in any concerted way. Your level of technical skills: Some approaches to implementing FAIR may require technical skills you may not yet feel comfortable with. While a lot is up to you, the first step is to evaluate how FAIR you think your data are: Exercise Thinking about a dataset you work with, complete the ARDC FAIR assessment . References and Resources \u00b6 https://www.nature.com/articles/sdata201618 Data Management Plans \u00b6 Learning Objectives Describe the purpose of a data management plan Describe the important elements of a data management plan Use a self-assessment to design a data management plan \"A data management plan or DMP is a formal document that outlines how data are to be handled both during a research project, and after the project is completed. [1] The goal of a data management plan is to consider the many aspects of data management, metadata generation, data preservation, and analysis before the project begins; this may lead to data being well-managed in the present, and prepared for preservation in the future.\" (Source: https://en.wikipedia.org/wiki/Data_management_plan ) Example DMP Why bother with a DMP? How would you answer? Do you have a data management plan? If so, how do you use it? \"Those who fail to plan, plan to fail\" Returning to the assertion that data (and its value) is at the foundation of your science, working without a data management plan should be considered scientific misconduct. Those are strong words. And while we might have an intuition of the boundaries of research ethics - data mismanagement seems more like an annoyance than misconduct. However, if your mismanagement leads to error in your research data, or the inability to make publicly-funded research open to the public, these are serious consequences. Increasingly, funders realize this. Stick: You have to make one Reviewers definitely look at them, but they may not be enforced. Carrot: Make your life easier Planning for you project makes it run more smoothly Avoid surprise costs Elements of a good DMP \u00b6 Information about data & data format(s) data types data sources analysis methods formats QA/QC version control data life cycle Metadata content and format(s) format standards Policies for access, sharing, and re-use funder obligations ethical and privacy issues (data justice) intellectual property, copyright, citation timeline for releases Long-term storage, data management, and preservation which data to preserve which archive/repository Budget( PAPPG ) each of the above elements cost time/money Personnel time for data preparation, management, documentation, and preservation (including time) Hardware and/or software for data management, back up, security, documentation, and preservation (including time) Publication/archiving costs (including time) Not only what, but who (roles). Extra challenges for collaborative projects. Machine actionable DMPs \u00b6 DMPs describe research methods that will evolve over the course of a project to be a useful tool for researchers and others, the content must be updated to capture the methods that are employed and the data that are produced (Source: https://doi.org/10.1371/journal.pcbi.1006750.g002 ) Tools for DMPs \u00b6 Exercise Thinking about a dataset you work with, complete the Data Stewardship Wizzard . References and Resources \u00b6 NSF Guidelines on DMPs https://dmptool.org/general_guidance https://dmptool.org/public_templates Professional and scholarly societies, e.g., theEcological Society of America http://www.esa.org/esa/science/data-sharing/resources-and-tools/ DataOne - https://dataoneorg.github.io/Education/bestpractices/ Data Carpentry - http://datacarpentry.org/ The US Geological Survey https://www.usgs.gov/data-management Repository registry (and search) service: http://www.re3data.org/ Your university library","title":"Managing Data"},{"location":"03_managing_data/#managing-data","text":"Learning Objectives After this lesson, you should be able to: Recognize data as the foundation of open science and be able to describe the \"life cycle of data\" Use self-assessments to evaluate your current data management practices Cite tools and resources to improve your data management practices Know the biggest challenge to effective data management","title":"Managing Data"},{"location":"03_managing_data/#why-should-you-care-about-data-management","text":"Most scientific work centers on generating new data or working with existing data, which means researchers spend a lot of time dealing with it. Ensuring that data are effectively organized, shared, and preserved is critical to making your science impactful, efficient, and open. How would you answer? If you give your data to a colleague who has not been involved with your project, would they be able to make sense of it? Would they be able to use it properly? If you come back to your own data in five years, will you be able to make sense of it? Will you be able to use it properly? When you are ready to publish a paper, is it easy to find all the correct versions of all the data you used and present them in a comprehensible manner? Data management skills produce self-describing datasets that: Make life much easier for you and your collaborators Benefit the scientific research community by allowing others to reuse your data Are required by most funders and many journals Recent Dear Colleague letter from NSF NSF proposal preparation guidelines","title":"Why should you care about data management?"},{"location":"03_managing_data/#data-self-assessment","text":"Part I: Basic questions Here are some questions about how you manage and work with data. We will complete some more formal assessments later, but for now let's see where you are. Activity In small groups, discuss the following questions. You will be provided with a space for documenting our shared answers. 1. What are the two or three data types that you most frequently work with? - Think about the sources (observational, experimental, simulated, compiled/derived) - Also consider the formats (tabular, sequence, database, image, etc.) 2. What is the scale of your data? Tip We often talk about the scale of data using the \"Three V's\" : - Volume: Size of the data (MBs, GBs, TBs); can also include how many files (e.g dozens of big files, or millions of small ones) - Velocity: How quickly are these data produced and analyzed? A lot coming in a single batch infrequently, or, a constant small amount of data that must be rapidly analyzed? - Variety: How many different data types (raw files? databases?) A fourth V (Veracity) captures the need to make decisions about data processing (i.e., separating low- and high-quality data) What is your strategy for storing and backing up your data? What is your strategy for verifying the integrity of your data? (i.e. verifying that your data has not be altered) What is your strategy for searching your data? What is your strategy for sharing (and getting credit for) your data? (i.e. How will do you share with your community/clients? How is that sharing documented? How do you evaluate the impact of data shared? )","title":"Data Self-assessment"},{"location":"03_managing_data/#data-management-basics","text":"Let's learn a little more about data so that we can evaluate your self-assessment responses.","title":"Data Management Basics"},{"location":"03_managing_data/#data-types","text":"Different types of data require different management practices. What are some data types and sources you might use in your work? (Adapted from DMP Tool Data management general guidance ) Data Types Text: field or laboratory notes, survey responses Numeric: tables, counts, measurements Audiovisual: images, sound recordings, video Models, computer code Discipline-specific: FASTA in biology, FITS in astronomy, CIF in chemistry Instrument-specific: equipment outputs Data Sources Observational Captured in real-time, typically outside the lab Usually irreplaceable and therefore the most important to safeguard Examples: Sensor readings, telemetry, survey results, images Experimental Typically generated in the lab or under controlled conditions Often reproducible, but can be expensive or time-consuming Examples: gene sequences, chromatograms, magnetic field readings Simulation Machine generated from test models Likely to be reproducible if the model and inputs are preserved Examples: climate models, economic models Derived / Compiled Generated from existing datasets Reproducible, but can be very expensive and time-consuming Examples: text and data mining, compiled database, 3D models Tip The Data Life Cycle Data management is the set of practices that allow researchers to effectively and efficiently handle data throughout the data life cycle. Although typically shown as a circle (below) the actually life cycle of any data item may follow a different path, with branches and internal loops. Being aware of your data's future helps you plan how to best manage them. Image from Strasser et al .","title":"Data Types"},{"location":"03_managing_data/#best-practices-for-the-data-life-cycle","text":"The most important thing to remember about data management is that you are not alone. There are data management experts within your discipline, many of them not farther away than your university library system. In addition, there are many organizations (including many cited below) which work to solve. Warning The biggest challenge to effective data management The biggest challenge to data management making it an afterthought. Unfortunately, poor data management doesn't have a high upfront cost. You can do substantial work before realizing you are in trouble. Like a swimmer in rip current, by the time you realize you are in trouble, you may already be close to drowning. The solution? Make data management the first thing you consider when starting a research project. It also needs to be a policy you institute right away for your research group. Here are some excellent steps to consider. The summary below is adapted from the excellent DataONE best practices primer . Plan Describe the data that will be compiled, and how the data will be managed and made accessible throughout its lifetime A good plan considers each of the stages below Collect Have a plan for data organization in place before collecting data Collect and store observation metadata at the same time you collect the metadata Take advantage of machine generated metadata Assure Record any conditions during collection that might affect the quality of the data Distinguish estimated values from measured values Double check any data entered by hand Perform statistical and graphical summaries (e.g., max/min, average, range) to check for questionable or impossible values. Mark data quality, outliers, missing values, etc. Describe: Comprehensive data documentation (i.e. metadata) is the key to future understanding of data. Without a thorough description of the context of the data, the context in which they were collected, the measurements that were made, and the quality of the data, it is unlikely that the data can be easily discovered, understood, or effectively used. Organize your data for publication. Before you can describe your data, you must decide how to organize them. This should be planned before hand, so that data organization is a minimal task at the time of publication. Thoroughly describe the dataset (e.g., name of dataset, list of files, date(s) created or modified, related datasets) including the people and organizations involved in data collection (e.g., authors, affiliations, sponsor). Also include: An ORCID (obtain one if you don't have one). The scientific context (reason for collecting the data, how they were collected, equipment and software used to generate the data, conditions during data collection, spatial and temporal resolution) The data themselves How each measurement was produced Units Format Quality assurance activities Precision, accuracy, and uncertainty Metadata standards and ontologies are invaluable for supporting data reuse. Metadata standards tell you: Which metadata attributes to include How to format your metadata What values are allowable for different attributes Some metadata standards you may want to consider: DataCite for publishing data Dublin Core for sharing data on the web MIxS Minimum Information for any (x) sequence OGC standards for geospatial data Ontologies provide standardization for metadata values: Example: Environment Ontology terms for the MIxS standards Example: Plant Ontology for plant tissue types or development stages FAIRSharing.org lists standards and ontologies for life sciences. The CyVerse Data Commons supports good data description through: Metadata templates Bulk metadata upload Automatic collection of analysis parameters, inputs, and outputs in the DE. Preserve In general, data must be preserved in an appropriate long-term archive (i.e. data center). Here are some examples: Sequence data should go to a national repository, frequently NCBI Identify data with value - it may not be necessary to preserve all data from a project The CyVerse Data Commons provides a place to publish and preserve data that was generated on or can be used in CyVerse, where no other repository exists. See lists of repositories at FAIRSharing.org See lists of repositories at Data Dryad Github repos can get DOIs through Zenodo Be aware of licensing and other intellectual property issues Repositories will require some kind of license, often the least restrictive (see for example Creative Commons ) Repositories are unlikely to enforce reuse restrictions, even if you apply them. Discover Good metadata allows you to discover your own data! Databases, repositories, and search indices provide ways to discover relevant data for reuse Google dataset search DataOne FAIRSharing.org Integrate Data integration is a lot of work Standards and ontologies are key to future data integration Know the data before you integrate them Don't trust that two columns with the same header are the same data Properly cite the data you reuse! Use DOIs ( Digital Object Identifiers ) wherever possible Analyze Follow open science principles for reproducible analyses (CyVerse, RStudio, notebooks, IDEs) State your hypotheses and analysis workflow before collecting data. Tools like Open Science Framework (OSF) allow you to make this public. Record all software, parameters, inputs, etc.","title":"Best practices for the data life cycle"},{"location":"03_managing_data/#references-and-resources","text":"DataOne best practices Center for Open Science","title":"References and Resources"},{"location":"03_managing_data/#fair-data","text":"Learning Objectives Recall the meaning of FAIR Understand why FAIR is a collection of principles (rather than rules) Use self-assessments to evaluate the FAIRness of your data","title":"FAIR Data"},{"location":"03_managing_data/#fair-principles","text":"In 2016, the FAIR Guiding Principles for scientific data management and stewardship were published in Scientific Data. Read it. Findable F1. (meta)data are assigned a globally unique and persistent identifier F2. data are described with rich metadata (defined by R1 below) F3. metadata clearly and explicitly include the identifier of the data it describes F4. (meta)data are registered or indexed in a searchable resource Accessible A1. (meta)data are retrievable by their identifier using a standardized communications protocol A1.1 the protocol is open, free, and universally implementable A1.2 the protocol allows for an authentication and authorization procedure, where necessary A2. metadata are accessible, even when the data are no longer available Interoperable I1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation. I2. (meta)data use vocabularies that follow FAIR principles I3. (meta)data include qualified references to other (meta)data Reusable R1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standard Tip Open vs. Public vs. FAIR: FAIR does not demand that data be open: See one definition of open: http://opendefinition.org/ Tip Why Principles? FAIR is a collection of principles. Ultimately, different communities within different scientific disciplines must work to interpret and implement these principles. Because technologies change quickly, focusing on the desired end result allows FAIR to be applied to a variety of situations now and in the foreseeable future.","title":"FAIR Principles"},{"location":"03_managing_data/#care-principles","text":"The CARE Principles for Indigenous Data Governance were drafted at the International Data Week and Research Data Alliance Plenary co-hosted event \"Indigenous Data Sovereignty Principles for the Governance of Indigenous Data Workshop,\" 8 November 2018, Gaborone, Botswana. Collective Benefit C1. For inclusive development and innovation C2. For improved governance and citizen engagement C3. For equitable outcomes Authority to Control A1. Recognizing rights and interests A2. Data for governance A3. Governance of data Responsibility R1. For positive relationships R2. For expanding capability and capacity R3. For Indigenous languages and worldviews Ethics E1. For minimizing harm and maximizing benefit E2. For justice E3. For future use","title":"CARE Principles"},{"location":"03_managing_data/#fair-tlc","text":"Traceable, Licensed, and Connected The need for metrics: https://zenodo.org/record/203295#.XkrzTxNKjzI","title":"FAIR - TLC"},{"location":"03_managing_data/#how-to-get-to-fair","text":"This is a question that only you can answer, that is because it depends on (among other things) Your scientific discipline: Your datatypes and existing standards for what constitutes acceptable data management will vary. The extent to which your scientific community has implemented FAIR: Some disciplines have significant guidelines on FAIR, while others have not addressed the subject in any concerted way. Your level of technical skills: Some approaches to implementing FAIR may require technical skills you may not yet feel comfortable with. While a lot is up to you, the first step is to evaluate how FAIR you think your data are: Exercise Thinking about a dataset you work with, complete the ARDC FAIR assessment .","title":"How to get to FAIR?"},{"location":"03_managing_data/#references-and-resources_1","text":"https://www.nature.com/articles/sdata201618","title":"References and Resources"},{"location":"03_managing_data/#data-management-plans","text":"Learning Objectives Describe the purpose of a data management plan Describe the important elements of a data management plan Use a self-assessment to design a data management plan \"A data management plan or DMP is a formal document that outlines how data are to be handled both during a research project, and after the project is completed. [1] The goal of a data management plan is to consider the many aspects of data management, metadata generation, data preservation, and analysis before the project begins; this may lead to data being well-managed in the present, and prepared for preservation in the future.\" (Source: https://en.wikipedia.org/wiki/Data_management_plan ) Example DMP Why bother with a DMP? How would you answer? Do you have a data management plan? If so, how do you use it? \"Those who fail to plan, plan to fail\" Returning to the assertion that data (and its value) is at the foundation of your science, working without a data management plan should be considered scientific misconduct. Those are strong words. And while we might have an intuition of the boundaries of research ethics - data mismanagement seems more like an annoyance than misconduct. However, if your mismanagement leads to error in your research data, or the inability to make publicly-funded research open to the public, these are serious consequences. Increasingly, funders realize this. Stick: You have to make one Reviewers definitely look at them, but they may not be enforced. Carrot: Make your life easier Planning for you project makes it run more smoothly Avoid surprise costs","title":"Data Management Plans"},{"location":"03_managing_data/#elements-of-a-good-dmp","text":"Information about data & data format(s) data types data sources analysis methods formats QA/QC version control data life cycle Metadata content and format(s) format standards Policies for access, sharing, and re-use funder obligations ethical and privacy issues (data justice) intellectual property, copyright, citation timeline for releases Long-term storage, data management, and preservation which data to preserve which archive/repository Budget( PAPPG ) each of the above elements cost time/money Personnel time for data preparation, management, documentation, and preservation (including time) Hardware and/or software for data management, back up, security, documentation, and preservation (including time) Publication/archiving costs (including time) Not only what, but who (roles). Extra challenges for collaborative projects.","title":"Elements of a good DMP"},{"location":"03_managing_data/#machine-actionable-dmps","text":"DMPs describe research methods that will evolve over the course of a project to be a useful tool for researchers and others, the content must be updated to capture the methods that are employed and the data that are produced (Source: https://doi.org/10.1371/journal.pcbi.1006750.g002 )","title":"Machine actionable DMPs"},{"location":"03_managing_data/#tools-for-dmps","text":"Exercise Thinking about a dataset you work with, complete the Data Stewardship Wizzard .","title":"Tools for DMPs"},{"location":"03_managing_data/#references-and-resources_2","text":"NSF Guidelines on DMPs https://dmptool.org/general_guidance https://dmptool.org/public_templates Professional and scholarly societies, e.g., theEcological Society of America http://www.esa.org/esa/science/data-sharing/resources-and-tools/ DataOne - https://dataoneorg.github.io/Education/bestpractices/ Data Carpentry - http://datacarpentry.org/ The US Geological Survey https://www.usgs.gov/data-management Repository registry (and search) service: http://www.re3data.org/ Your university library","title":"References and Resources"},{"location":"04_documentation_communication/","text":"Documentation & Communication \u00b6 Learning Objectives After this lesson, you should be able to: Identify and explain different types of project documentation (both internal and external) Describe tools and approaches to creating your own documentation Describe best practices for maintaining documentation Identify and explain different communication strategies for working in a team (virtual and in person) Create your own GitHub Pages website (!) Peer-reviewed manuscripts or conference preceedings / presentations /posters are one of the primary ways of communicating science, but they are far from the only avenues of communcation available to us as researchers and educators. As our methods become more complicated and customized, open science means giving people a better understanding of our approaches and tools than may be required in most journals. Communicating amongst a team of researchers that may span institutions, time zones, or continents also requires more modern approaches. Strong frameworks for internal communication and documentation can make collaboration easier, improve the quality of your science, and reduce the hassle of collaborative work. Project Documentation \u00b6 This website is rendered using GitHub Pages using MkDocs and the Material theme for MkDocs. Other popular website generators for GitHub Pages are Jekyll Theme or Bootstrap.js . ReadTheDocs.org has become a popular tool for developing web-based documentation. Think of RTD as \"Continuous Documentation\". Bookdown is an open-source R package that facilitates writing books and long-form articles/reports with R Markdown. Quarto is an open-source scientific and technical publishing system built on Pandoc Confluence Wikis (CyVerse) are another tool for documenting your workflow. Things to remember about Documentation Documentation should be written in such a way that people who did not write the documentation can read and then use or read and then teach others in the applications of the material. Documentation is best treated as a living document, but version control is necessary to maintain it Technology changes over time, expect to refresh documentation every 3-5 years as your projects age and progress. GitHub Pages You can pull templates from other GitHub users for your website, e.g. Jekyll themes GitHub pages are free, fast, and easy to build, but limited in use of subdomain or URLs. ReadTheDocs publishing websites via ReadTheDocs.com costs money. You can work in an offline state, where you develop the materials and publish them to your localhost using Sphinx You can work on a website template in a GitHub repository, and pushes are updated in near real time using ReadTheDocs.com. Material MkDocs publish via GitHub Actions Uses open source Material or ReadTheDocs Themes Bookdown Bookdown websites can be hosted by RStudio Connect You can publish a Bookdown website using Github Pages Quarto Build a website using Quarto's template builder Build with Github Pages JupyterBook Based on Project Jupyter ipynb and MarkDown Uses conda package management GitBook GitBook websites use MarkDown syntax Free for open source projects, paid plans are available Websites to Host Methods & Protocols \u00b6 Open Science Framework for free. OSF can be directly linked to your ORCID. Integrated project management tools Uses templates to create a project website Can publish preprints from within project management tools Protocols.io - collaborative platform and preprint server for: science methods, computational workflows, clinical trials, operational procedures, safety checklists, and instructions / manuals. QUBES - community of math and biology educators who share resources and methods for preparing students to tackle real, complex, biological problems. Hands On: Build a GitHub Pages Website \u00b6 Create a GitHub account Clone the repo https://github.com/username/username.github.io Create an index.html Push it back to GitHub ReadTheDocs.org Install Use Github Create a ReadTheDocs account Material MkDocs Install Material a. use a reqirements.txt b. or pip install mkdocs-material Clone a repository with an existing template or create a new repo with mkdocs new . Run python -m mkdocs serve to build and serve locally Open your browser to preview the build at https://localhost:8000 ` Bookdown Install R and RStudio Install Bookdown package with install.packages(\"bookdown\", dependencies=TRUE) Open the Bookdown demo and get started Quarto Build locally Push to GitHub (alt) Use GitHub Actions JupyterBook Create your first book GitBook Follow Template builder Communication \u00b6 Internal Project \u00b6 Choosing which software to use for your internal lab communication can be complicated by the cost of setting up, the cost of maintaining, and simply by the sheer number of platforms that are out there. For this workshop, we use SLACK (Searchable Log of All Conversation & Knowledge). Microsoft's competitor to SLACK is Microsoft Teams . Remember, the intention of these platforms are to improve productivity & not become a distraction. SLACK Slack has plenty of apps for coordinating multiple services, i.e. Calendars, Github, GoogleDrive, Box, etc. Slack is limiting unless you're willing to pay for the professional support. Microsoft Teams Teams is used by many R1 research universities as part of their campus wide license agreement for Office 365 Business and Education. Other popular alternatives BaseCamp Discord Mastodon Mattermost Useful links for creating a SLACK workspace Create a new Workspace Create channels, add apps & tools External (Public) \u00b6 Although we didn't cover it explicitly in the announcement for the workshop, communicating with the public and other members of your science community is one of the most important parts of your science! There are many ways scientists use social media and the web to share their data science ideas: \"Science Twitter\" - is really just regular Twitter , but with a focus on following other scientists and organizations, and tweeting about research you're interested in. By building up a significant following, more people will know you, know about your work, and you'll have a higher likelihood of meeting other new collaborators. Blogs - there are numerous platforms for blogging about research, the older platforms tend to dominate this space. Other platforms like, Medium offer a place for reseachers to create personalized reading spaces and self publish. Community groups - There are lists (and lists of lists) of nationals research organizations , in which a researcher can become involved. These older organziations still rely on official websites, science journal blogs, and email lists to communicate with their members. In the earth sciences there are open groups which focus on communication like the Earth Science Information Partners (ESIP) with progressive ideas about how data and science can be done. Other groups, like The Carpentries and Research Bazaar are focused on data science training and digital literacy. Important Remember Personal and Professional Accounts are Not Isolated You decide what you post on the internet. Your scientist identity may be a part of your personal identity on social media, it might be separate. A future employer or current employer can see your old posts. What you post in your personal accounts can be considered a reflection of the organization you work for and may be used in decisions about hiring or dismissal. Self Assessment \u00b6 What are the benefits of using a GitHub.io website? Github Pages are hosted directly from your GitHub repository. Just edit, push, and your changes are live. You do not need to run your own web server!! Self-Paced Material \u00b6 15 Data Science Communities to Join Python & Slack Slack CLI notifications Meetups","title":":material-file-document-multiple: Documentation & :material-antenna: Communication"},{"location":"04_documentation_communication/#documentation-communication","text":"Learning Objectives After this lesson, you should be able to: Identify and explain different types of project documentation (both internal and external) Describe tools and approaches to creating your own documentation Describe best practices for maintaining documentation Identify and explain different communication strategies for working in a team (virtual and in person) Create your own GitHub Pages website (!) Peer-reviewed manuscripts or conference preceedings / presentations /posters are one of the primary ways of communicating science, but they are far from the only avenues of communcation available to us as researchers and educators. As our methods become more complicated and customized, open science means giving people a better understanding of our approaches and tools than may be required in most journals. Communicating amongst a team of researchers that may span institutions, time zones, or continents also requires more modern approaches. Strong frameworks for internal communication and documentation can make collaboration easier, improve the quality of your science, and reduce the hassle of collaborative work.","title":" Documentation &amp;  Communication"},{"location":"04_documentation_communication/#project-documentation","text":"This website is rendered using GitHub Pages using MkDocs and the Material theme for MkDocs. Other popular website generators for GitHub Pages are Jekyll Theme or Bootstrap.js . ReadTheDocs.org has become a popular tool for developing web-based documentation. Think of RTD as \"Continuous Documentation\". Bookdown is an open-source R package that facilitates writing books and long-form articles/reports with R Markdown. Quarto is an open-source scientific and technical publishing system built on Pandoc Confluence Wikis (CyVerse) are another tool for documenting your workflow. Things to remember about Documentation Documentation should be written in such a way that people who did not write the documentation can read and then use or read and then teach others in the applications of the material. Documentation is best treated as a living document, but version control is necessary to maintain it Technology changes over time, expect to refresh documentation every 3-5 years as your projects age and progress. GitHub Pages You can pull templates from other GitHub users for your website, e.g. Jekyll themes GitHub pages are free, fast, and easy to build, but limited in use of subdomain or URLs. ReadTheDocs publishing websites via ReadTheDocs.com costs money. You can work in an offline state, where you develop the materials and publish them to your localhost using Sphinx You can work on a website template in a GitHub repository, and pushes are updated in near real time using ReadTheDocs.com. Material MkDocs publish via GitHub Actions Uses open source Material or ReadTheDocs Themes Bookdown Bookdown websites can be hosted by RStudio Connect You can publish a Bookdown website using Github Pages Quarto Build a website using Quarto's template builder Build with Github Pages JupyterBook Based on Project Jupyter ipynb and MarkDown Uses conda package management GitBook GitBook websites use MarkDown syntax Free for open source projects, paid plans are available","title":" Project Documentation"},{"location":"04_documentation_communication/#websites-to-host-methods-protocols","text":"Open Science Framework for free. OSF can be directly linked to your ORCID. Integrated project management tools Uses templates to create a project website Can publish preprints from within project management tools Protocols.io - collaborative platform and preprint server for: science methods, computational workflows, clinical trials, operational procedures, safety checklists, and instructions / manuals. QUBES - community of math and biology educators who share resources and methods for preparing students to tackle real, complex, biological problems.","title":"Websites to Host Methods &amp; Protocols"},{"location":"04_documentation_communication/#hands-on-build-a-github-pages-website","text":"Create a GitHub account Clone the repo https://github.com/username/username.github.io Create an index.html Push it back to GitHub ReadTheDocs.org Install Use Github Create a ReadTheDocs account Material MkDocs Install Material a. use a reqirements.txt b. or pip install mkdocs-material Clone a repository with an existing template or create a new repo with mkdocs new . Run python -m mkdocs serve to build and serve locally Open your browser to preview the build at https://localhost:8000 ` Bookdown Install R and RStudio Install Bookdown package with install.packages(\"bookdown\", dependencies=TRUE) Open the Bookdown demo and get started Quarto Build locally Push to GitHub (alt) Use GitHub Actions JupyterBook Create your first book GitBook Follow Template builder","title":"Hands On: Build a GitHub Pages Website"},{"location":"04_documentation_communication/#communication","text":"","title":" Communication"},{"location":"04_documentation_communication/#internal-project","text":"Choosing which software to use for your internal lab communication can be complicated by the cost of setting up, the cost of maintaining, and simply by the sheer number of platforms that are out there. For this workshop, we use SLACK (Searchable Log of All Conversation & Knowledge). Microsoft's competitor to SLACK is Microsoft Teams . Remember, the intention of these platforms are to improve productivity & not become a distraction. SLACK Slack has plenty of apps for coordinating multiple services, i.e. Calendars, Github, GoogleDrive, Box, etc. Slack is limiting unless you're willing to pay for the professional support. Microsoft Teams Teams is used by many R1 research universities as part of their campus wide license agreement for Office 365 Business and Education. Other popular alternatives BaseCamp Discord Mastodon Mattermost Useful links for creating a SLACK workspace Create a new Workspace Create channels, add apps & tools","title":"Internal Project"},{"location":"04_documentation_communication/#external-public","text":"Although we didn't cover it explicitly in the announcement for the workshop, communicating with the public and other members of your science community is one of the most important parts of your science! There are many ways scientists use social media and the web to share their data science ideas: \"Science Twitter\" - is really just regular Twitter , but with a focus on following other scientists and organizations, and tweeting about research you're interested in. By building up a significant following, more people will know you, know about your work, and you'll have a higher likelihood of meeting other new collaborators. Blogs - there are numerous platforms for blogging about research, the older platforms tend to dominate this space. Other platforms like, Medium offer a place for reseachers to create personalized reading spaces and self publish. Community groups - There are lists (and lists of lists) of nationals research organizations , in which a researcher can become involved. These older organziations still rely on official websites, science journal blogs, and email lists to communicate with their members. In the earth sciences there are open groups which focus on communication like the Earth Science Information Partners (ESIP) with progressive ideas about how data and science can be done. Other groups, like The Carpentries and Research Bazaar are focused on data science training and digital literacy. Important Remember Personal and Professional Accounts are Not Isolated You decide what you post on the internet. Your scientist identity may be a part of your personal identity on social media, it might be separate. A future employer or current employer can see your old posts. What you post in your personal accounts can be considered a reflection of the organization you work for and may be used in decisions about hiring or dismissal.","title":"External (Public)"},{"location":"04_documentation_communication/#self-assessment","text":"What are the benefits of using a GitHub.io website? Github Pages are hosted directly from your GitHub repository. Just edit, push, and your changes are live. You do not need to run your own web server!!","title":"Self Assessment"},{"location":"04_documentation_communication/#self-paced-material","text":"15 Data Science Communities to Join Python & Slack Slack CLI notifications Meetups","title":"Self-Paced Material"},{"location":"05_version_control/","text":"Version Control \u00b6 Version control refers to keeping track of the version of a file, set of files, or a whole project. Some version control tools: Microsoft Office's Track Changes functionality Apple's Time Machine Google Docs' Version History Git Version control is as much a philosophy as a set of tools; you don't need to master Git to utilize version control (though it is certainly a worthwhile tool for many researchers). Definitions \u00b6 Git-related Definitions Git - tool for version control. GitHub - hosted server that is also interactive. repo - short for repository local - on your personal computer. remote - somewhere other than your computer. GitHub can host remote repositories. clone - copy of a repository that lives locally on your computer. Pushing changes will affect the repository online. fetch - getting latest changes to the repository on your local computer. branch - a history of changes to a repository. You can have parallel branches with separate histories, allowing you to keep a \"main\" version and development versions. fork - copy of someone else's repository stored locally on your account. From forks, you can make pull requests to the main branch. upstream - primary or main branch of original repository. downstream - branch or fork of repository. commit - finalize a change. push - add changes back to the remote repository. merge - takes changes from a branch or fork and applies them to the main. pull request - proposed changes to/within a repository. issue - suggestions or tasks needed for the repository. Allows you to track decisions, bugs with the repository, etc. Git vs. GitHub \u00b6 Git is a command-line program for version control of repositories. It keeps track of changes you make to files in your repository and stores those changes in a .git folder in that repository. These changes happen whenever you make a commit . Git stores the history of these commits in a \"tree\", so you can go back to any previous commit. By keeping track of the differences between commits, Git can be much more efficient than storing an entire copy of each version in a document's history. You could utilize Git completely on its own, on your local computer, and get a lot of benefits. You will have a history of the changes you made to a project, allowing you to go back to any old version of your work. However, where Git really shines is in collaborative work. In order to effectively collaborate with others on a project, you need two basic features: a way to allow people to work in parallel, and a way to host repositories somewhere where everyone can access them. The first feature is branching , which is part of Git, and the hosting part can be taken care of by platforms like GitHub, GitLab, or Bitbucket. We will focus on GitHub. GitHub is a site that can remotely host your Git repositories. By putting your repository onto GitHub, you get a backup of the repository, a way to collaborate with others, and a lot of other features. Practical Git Techniques \u00b6 After learning the basics of using Git, which you can learn with the Software Carpentry Git Lesson , there are some next things that can be useful to learn. Here are a couple topics that are worth digging into more: Using the Git log you can access using git log will show you your commit history useful for figuring out where you need to roll back to Reverting there are a lot of different ways to \"undo\" something in Git some are safer, some are a bit riskier depends on what stage of the commit process you're in here are some useful resources : https://www.codementor.io/@citizen428/git-tutorial-10-common-git-problems-and-how-to-fix-them-aajv0katd http://justinhileman.info/article/git-pretty/git-pretty.png https://github.blog/2015-06-08-how-to-undo-almost-anything-with-git/ Branching this is important to learn if you\\'re going to be doing any sort of collaboration here is a fantastic resource for learning how git branching really works: https://learngitbranching.js.org/ you will probably have to deal with merge conflicts at some point merge conflicts happen when two branches are being merged, but they have different changes to the same part of a file perhaps you are working on a feature branch, and you change line 61 in file.R , but someone else made a change to the main branch at line 61 in file.R . When you try to merge the feature and main branches, Git won't know which changes to line 61 in file.R are correct, and you will need to manually decide. here are some good resources: https://docs.github.com/en/github/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line https://nitaym.github.io/ourstheirs/ .gitignore you often want Git to completely ignore certain files generated files (like HTML files from Markdown docs) IDE-specific files like in .RStudio or .vscode folders really big files, like data or images if you accidentally commit a really big file, GitHub might not let you push that commit if you have a huge file in Git, your repository size can get way too big this is a pain to solve, so use the .gitignore file ahead of time, but if you need to fix this, here is a great resource: https://necromuralist.github.io/posts/removing-large-files-from-git-using-bfg-and-a-local-repository/ Useful GitHub Features \u00b6 At its core, GitHub is just a place to host your Git repositories. However, it offers a lot of functionality that has less to do with Git, and more to do with our favorite topic, Project Management . We will walk through a few of these useful features. Issues issues let you plan out changes and suggestions to a repo closing/reopening labels assigning templates numbering/mentioning GitHub documentation: https://docs.github.com/en/issues Pull Requests pull requests are a way to request merging code from one branch to another typical workflow is for someone to fork a repo, then make a PR from that repo to another reviews commenting merging closing issues GitHub documentation: https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests Organizations you can use Organizations to organize sets of repositories roles teams GitHub documentation: https://docs.github.com/en/organizations Other neat things Permissions/collaborators GitHub Classroom Gists CSV and map rendering Code editor Note Git is not really for storing or manipulating data, especially large files. But the CyVerse Discovery Environment is a great place to serve, store, and share data.","title":"Version Control"},{"location":"05_version_control/#version-control","text":"Version control refers to keeping track of the version of a file, set of files, or a whole project. Some version control tools: Microsoft Office's Track Changes functionality Apple's Time Machine Google Docs' Version History Git Version control is as much a philosophy as a set of tools; you don't need to master Git to utilize version control (though it is certainly a worthwhile tool for many researchers).","title":"Version Control"},{"location":"05_version_control/#definitions","text":"Git-related Definitions Git - tool for version control. GitHub - hosted server that is also interactive. repo - short for repository local - on your personal computer. remote - somewhere other than your computer. GitHub can host remote repositories. clone - copy of a repository that lives locally on your computer. Pushing changes will affect the repository online. fetch - getting latest changes to the repository on your local computer. branch - a history of changes to a repository. You can have parallel branches with separate histories, allowing you to keep a \"main\" version and development versions. fork - copy of someone else's repository stored locally on your account. From forks, you can make pull requests to the main branch. upstream - primary or main branch of original repository. downstream - branch or fork of repository. commit - finalize a change. push - add changes back to the remote repository. merge - takes changes from a branch or fork and applies them to the main. pull request - proposed changes to/within a repository. issue - suggestions or tasks needed for the repository. Allows you to track decisions, bugs with the repository, etc.","title":"Definitions"},{"location":"05_version_control/#git-vs-github","text":"Git is a command-line program for version control of repositories. It keeps track of changes you make to files in your repository and stores those changes in a .git folder in that repository. These changes happen whenever you make a commit . Git stores the history of these commits in a \"tree\", so you can go back to any previous commit. By keeping track of the differences between commits, Git can be much more efficient than storing an entire copy of each version in a document's history. You could utilize Git completely on its own, on your local computer, and get a lot of benefits. You will have a history of the changes you made to a project, allowing you to go back to any old version of your work. However, where Git really shines is in collaborative work. In order to effectively collaborate with others on a project, you need two basic features: a way to allow people to work in parallel, and a way to host repositories somewhere where everyone can access them. The first feature is branching , which is part of Git, and the hosting part can be taken care of by platforms like GitHub, GitLab, or Bitbucket. We will focus on GitHub. GitHub is a site that can remotely host your Git repositories. By putting your repository onto GitHub, you get a backup of the repository, a way to collaborate with others, and a lot of other features.","title":"Git vs. GitHub"},{"location":"05_version_control/#practical-git-techniques","text":"After learning the basics of using Git, which you can learn with the Software Carpentry Git Lesson , there are some next things that can be useful to learn. Here are a couple topics that are worth digging into more: Using the Git log you can access using git log will show you your commit history useful for figuring out where you need to roll back to Reverting there are a lot of different ways to \"undo\" something in Git some are safer, some are a bit riskier depends on what stage of the commit process you're in here are some useful resources : https://www.codementor.io/@citizen428/git-tutorial-10-common-git-problems-and-how-to-fix-them-aajv0katd http://justinhileman.info/article/git-pretty/git-pretty.png https://github.blog/2015-06-08-how-to-undo-almost-anything-with-git/ Branching this is important to learn if you\\'re going to be doing any sort of collaboration here is a fantastic resource for learning how git branching really works: https://learngitbranching.js.org/ you will probably have to deal with merge conflicts at some point merge conflicts happen when two branches are being merged, but they have different changes to the same part of a file perhaps you are working on a feature branch, and you change line 61 in file.R , but someone else made a change to the main branch at line 61 in file.R . When you try to merge the feature and main branches, Git won't know which changes to line 61 in file.R are correct, and you will need to manually decide. here are some good resources: https://docs.github.com/en/github/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line https://nitaym.github.io/ourstheirs/ .gitignore you often want Git to completely ignore certain files generated files (like HTML files from Markdown docs) IDE-specific files like in .RStudio or .vscode folders really big files, like data or images if you accidentally commit a really big file, GitHub might not let you push that commit if you have a huge file in Git, your repository size can get way too big this is a pain to solve, so use the .gitignore file ahead of time, but if you need to fix this, here is a great resource: https://necromuralist.github.io/posts/removing-large-files-from-git-using-bfg-and-a-local-repository/","title":"Practical Git Techniques"},{"location":"05_version_control/#useful-github-features","text":"At its core, GitHub is just a place to host your Git repositories. However, it offers a lot of functionality that has less to do with Git, and more to do with our favorite topic, Project Management . We will walk through a few of these useful features. Issues issues let you plan out changes and suggestions to a repo closing/reopening labels assigning templates numbering/mentioning GitHub documentation: https://docs.github.com/en/issues Pull Requests pull requests are a way to request merging code from one branch to another typical workflow is for someone to fork a repo, then make a PR from that repo to another reviews commenting merging closing issues GitHub documentation: https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests Organizations you can use Organizations to organize sets of repositories roles teams GitHub documentation: https://docs.github.com/en/organizations Other neat things Permissions/collaborators GitHub Classroom Gists CSV and map rendering Code editor Note Git is not really for storing or manipulating data, especially large files. But the CyVerse Discovery Environment is a great place to serve, store, and share data.","title":"Useful GitHub Features"},{"location":"06_reproducibility_i/","text":"Repeatability and Reproducibility \u00b6 The so-called reproducibility crisis (see 1 , 2 , 3 ) is something you have probably heard about (and maybe one of the reasons you have come to FOSS). Headlines in the media (such as Most scientists can't replicate studies by their peers ) definitely give pause to researchers and ordinary citizens who hope that the science used to recommend a course of medical treatment or design self-driving cars is sound. Before we go further, it's actually important to ask what is reproducibility? Question How do you define reproducible science? Answer In Reproducibility vs. Replicability , Hans Plesser gives the following useful definitions: Repeatability (Same team, same experimental setup): The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation. Replicability (Different team, same experimental setup): The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author's own artifacts. Reproducibility (Different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently. The paper goes on to further simplify: Methods reproducibility : provide sufficient detail about procedures and data so that the same procedures could be exactly repeated. Results reproducibility : obtain the same results from an independent study with procedures as closely matched to the original study as possible. Inferential reproducibility : draw the same conclusions from either an independent replication of a study or a reanalysis of the original study. Defining Reproducibility \u00b6 Discussion Question How do these definitions apply to your research/teaching? Work with your fellow learners to develop a shortlist of ways reproducibility relates to your work. Try to identify challenges and even successes you'd like to share. Often, when we say \"reproducibility\" we mean all or at least several of the concepts the proceeding discussion encompasses. Really, reproducibility can be thought of as set values such as some laboratories express in a code of conduct, see for example Ross-Ibarra Lab code of conduct or Bahlai Lab Policies . Reproducibility comes from our obligations and desires to work ethically, honestly, and with confidence that the data and knowledge we produce is done has integrity. Reproducibility is also a \"spectrum of practices\", not a single step. (See figure below from Peng, 2011 ). Assuming you have taken in the potentially anxiety inducing information above, the most important thing to know is that there is a lot of help to make reproducibility a foundation of all of your research. Repeatability: a first step \u00b6 A big first step on the road to reproducibility is repeatability . In the context of computation, this means that you should be able to reliably generate the same results. In many ways, this is the biggest hurdle to reproducibility, as it often requires the biggest leap in skills. You can think of repeatability in a few ways. Discussion Question Have you ever had any hurdles to reproducing your work? Have you ever run into a problem that prevented you from generating the same results, figures, analyses as before? Have you ever lost time trying to figure out how you (or a collaborator) got a particular result? What were the issues you ran into, and how might you have solved them? Software Management \u00b6 Have you ever tried to run a script, only to realize you had updated a package without knowing, and now the script doesn't work? Package managers can be extremely helpful in keeping software versions aligned with projects. In Python, it is common to use pip and a requirements.txt file, and in R, the renv package can be used to keep package versions stable within individual projects. Automation \u00b6 In the process of making your work more repeatable, you will often be trying to reduce the amount of work you're doing \"by hand\". Reducing the human input necessary at each step of a project is a key to reliably reproducing the same results, but it can also help save you a lot of time in the long run. Have you ever manually edited a figure for a manuscript, only to be asked to change something that negated all your manual edits? Well, in the short run, it may have been quicker to just tinker with the graph by hand, but in the long run, figuring out how to use code to generate the whole thing would have saved you time. Automating tasks often comes with an up-front cost, but it is important for the eventual reproducibility of the work, and will often save you time in the short run. Automation also tends to make tasks scale more easily (editing 10 rows of data by hand is fine, editing 10,000 is much harder), adapt to new scenarios, and extend to future projects. Discussion Question What are some tasks you have automated or want to automate? Have you ever successfully automated a task? Found a way to make something scale or take less time? What was the task, and how did you do it? Are there any things you wish you could automate? What are some barriers to automating them? While we often think about writing scripts to clean data, run analyses, and generate figures, there are even more parts of a research project that can be automated. Here are a few examples: data validation model checking/validation software installation report/manuscript generation citation management email/GitHub/Slack notifications workflow itself (using things like make, Snakemake, Nextflow, targets) Code can be thought of as a set of machine-actionable instructions, or instructions that we write for a computer to follow. What other sets of instructions do you have, either written down or in your head? How can you turn them into something machine-actionable? Disposability \u00b6 A great approach to repeatability/reproducbility is to ask \"could I generate my results if I lost X?\" What might happen to your work if: you changed some code and your script broke? you couldn't find a figure when a journal asked for it? some software got uninstalled from your computer? your laptop got stolen? some software or computing provider stopped being maintained? Get off your own machine \u00b6 More and more work is being done somewhere other than a personal computer. This could be an HPC cluster at a university or a cloud computing provider. \"Cloud\" just means somebody else is handling the computers, and you get to use them when you need to, typically for a price. Non-local computing resources have varying levels of complexity, flexibility, cost, and scale. Some services like Binder, Colab, and VICE try to abstract more of the computational details away, letting you focus on your code (ideally). Others, like Gitpod, Codespaces, or GitHub Actions, have more limited uses (in a good way). Dependency Hell \u00b6 Think for a moment about all the branching possibilities for how a computer could be set up: hardware: CPUs, GPUs, RAM Operating system: many flavors of Linux, MacOS, Windows Software versions: R, Python, etc. Package versions: specific R or Python packages, etc. Simply trying to get the same setup as anyone else is difficult enough, but you can also run into all sorts of dependencies. Let's say you try to update a package to match the version someone else used for a project. However, after updating it, you realize you need to update 3 other packages. After that, you realize you need a newer version of R. You finally manage to get everything set up, but when you go back to a different project the next week, nothing works! All those updates made your code for your other project break. You spend a week fixing your code to work with the newer software, and you're finally done... but now your advisor gives you a dataset 10x the size and says you'll need to run it on the cloud. You throw your laptop out the window and move to the woods to live the life of a hermit. All jokes aside, dealing with software dependencies can be extremely frustrating, and so can setting stuff up on a remote location. It can be even more frustrating if you're trying to reproduce results but you don't actually know the entire software stack used to generate them. There is a way to handle all of these frustrations at once: Containers \u00b6 Ok, to be fair, working with containers will also be frustrating. But the beautiful thing about working with containers is that you can handle all of the hard stuff at the start of a project, and you won't have to worry about things changing later on. What are containers? Containers are reproducible computing environments that contain an operating system (OS), software, and even code needed to run analyses. Containers are similar to virtual machines (VMs), but are smaller and easier to share. A big distinction between Containers and VMs is what is within each environment: VMs require the OS to be present within the image, whilst containers rely solemnly on the host OS (and the container engine). Source: Microsoft Cloudblogs A popular container platform is Docker ( wikipedia , \"what is a Docker container?\" ), hosting user created containers on DockerHub , and providing a cross-OS user-friendly toolset for container creation and deployment. RStudio has a number of available Docker containers , each for different use cases and maintained by the Rocker Project . Apptainer (formerly, Singularity), is another popular container engine, which allows you to deploy containers on HPC clusters. Reproducibility tutorial I: Setting up your project \u00b6 This section is going to cover a short tutorial spanning this and the next reproducibility sessions. In this tutorial you're going to use software and tools discussed today and previously in FOSS. What you'll be using: GitHub (already installed) Docker (already installed) Snakemake Conda JetStream2 Connecting to JetStream2 \u00b6 JetStream2 is a service that allows users to create and connect to Virtual Machines (VMs). We have created a number of virtual machines for you to connect to and follow this tutorial, however, you're welcome to use your own machine for this. Warning For security reasons, the google sheets with the IP addresses and passwords is shared solemnly through the HackMD. In order to connect to JetStream2, you will be using the SSH (Secure Shell) protocol, which allows connections over a network. To connect to a machine, open a termial and enter your user number and IP as follows and press enter: ssh <user>@<IP> When asked for the password, copy and paste the corresponding password from the google sheets and press enter. Note : you will NOT see the password being pasted (but it's there!) Install Conda \u00b6 Conda is a popular tool for installing software. Typically software you want to use requires other software (dependancies) to be installed. Conda can manage all of this for you. Each available Conda package is part of a \u201crecipe\u201d that includes everything you need to run your software. There are different versions of Conda, including some specific for bioinformatics like Bioconda . We will install Conda and then use it to install some of the tools we need. We will install a lightweight version of Conda called MiniConda . # Download conda and add right permissions wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh chmod +x Miniconda3-py39_4.12.0-Linux-x86_64.sh # install conda silenty (-b) and update (-u) and initial conda run ./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -u ~/miniconda3/bin/conda init # Restart bash so that conda is activated source ~/.bashrc You'll be able to tell when conda is active when next (base) is present next to the to the shell prompt such as (base) user@machine Conda should now be installed and can be used to install other necessary packages! With this in mind, we are going to create our own environment (select y when prompted). conda create --name myenv Activate your new environment with conda activate myenv You can see the list of environments you can activate by doing conda env list Installing packages through conda \u00b6 Conda makes installing packages simple. Due to it's widespread use, conda has a large number of widely available packages; You can search for these in at https://anaconda.org/ . We are going to to use conda to install snakemake. # See what snakemake version are available conda search -c bioconda snakemake # Let's choose the latest, in this case 7.16.0 conda install -c bioconda -c conda-forge -y snakemake=7.16.0 conda will proceed and install all the dependencies required by snakemake # verify the installation snakemake --version You can view the installed conda packages by doing conda list In order to make your environment reproducible, conda allows you to export your environment. conda env export > myenv.yml GitHub repository setup and documentation \u00b6 Create a repository on GitHub to document your work: On GitHub , navigate to your account page and create a new repository (add a README to create structure!) Clone your repository to the VM with <repository_url>.git (find the url under the green Code button) Add your history to the README file with history >> README.md and add meaninigful comments. .md is the MarkDown extension that is used for formatting in GitHub (and HackMD!); Read more about here . A well documented document may look similar to: # reproducibility-tutorial ## Computer setup ### Download conda and add right permissions wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh chmod +x Miniconda3-py39_4.12.0-Linux-x86_64.sh ### install conda silenty (-b), update (-u) and initial start ./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -u ~/miniconda3/bin/conda init ### Restart bash so that conda is activated source ~/.bashrc ### See what snakemake version are available conda search -c bioconda snakemake ### Let's choose the 7.16.0 conda install -c bioconda -c conda-forge -y snakemake=7.16.0 ### verify the installation snakemake --version ### Exported conda environment conda env export > myenv.yml ### Cloned git repository git clone https://github.com/<user>/<repository>.git Add, commit and push your changes git add . git commit -m \"adding initial documentation\" git push Github will ask for you username and password; When asked about the password, input a GitHub token. To create a token go to Account > Settings > Developer settings > Personal access tokens > Generate new token , add a note, select all the necessary permissions and select Generate token; Copy the token and use it as password!","title":"Repeatability and Reproducibility"},{"location":"06_reproducibility_i/#repeatability-and-reproducibility","text":"The so-called reproducibility crisis (see 1 , 2 , 3 ) is something you have probably heard about (and maybe one of the reasons you have come to FOSS). Headlines in the media (such as Most scientists can't replicate studies by their peers ) definitely give pause to researchers and ordinary citizens who hope that the science used to recommend a course of medical treatment or design self-driving cars is sound. Before we go further, it's actually important to ask what is reproducibility? Question How do you define reproducible science? Answer In Reproducibility vs. Replicability , Hans Plesser gives the following useful definitions: Repeatability (Same team, same experimental setup): The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation. Replicability (Different team, same experimental setup): The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author's own artifacts. Reproducibility (Different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently. The paper goes on to further simplify: Methods reproducibility : provide sufficient detail about procedures and data so that the same procedures could be exactly repeated. Results reproducibility : obtain the same results from an independent study with procedures as closely matched to the original study as possible. Inferential reproducibility : draw the same conclusions from either an independent replication of a study or a reanalysis of the original study.","title":"Repeatability and Reproducibility"},{"location":"06_reproducibility_i/#defining-reproducibility","text":"Discussion Question How do these definitions apply to your research/teaching? Work with your fellow learners to develop a shortlist of ways reproducibility relates to your work. Try to identify challenges and even successes you'd like to share. Often, when we say \"reproducibility\" we mean all or at least several of the concepts the proceeding discussion encompasses. Really, reproducibility can be thought of as set values such as some laboratories express in a code of conduct, see for example Ross-Ibarra Lab code of conduct or Bahlai Lab Policies . Reproducibility comes from our obligations and desires to work ethically, honestly, and with confidence that the data and knowledge we produce is done has integrity. Reproducibility is also a \"spectrum of practices\", not a single step. (See figure below from Peng, 2011 ). Assuming you have taken in the potentially anxiety inducing information above, the most important thing to know is that there is a lot of help to make reproducibility a foundation of all of your research.","title":"Defining Reproducibility"},{"location":"06_reproducibility_i/#repeatability-a-first-step","text":"A big first step on the road to reproducibility is repeatability . In the context of computation, this means that you should be able to reliably generate the same results. In many ways, this is the biggest hurdle to reproducibility, as it often requires the biggest leap in skills. You can think of repeatability in a few ways. Discussion Question Have you ever had any hurdles to reproducing your work? Have you ever run into a problem that prevented you from generating the same results, figures, analyses as before? Have you ever lost time trying to figure out how you (or a collaborator) got a particular result? What were the issues you ran into, and how might you have solved them?","title":"Repeatability: a first step"},{"location":"06_reproducibility_i/#software-management","text":"Have you ever tried to run a script, only to realize you had updated a package without knowing, and now the script doesn't work? Package managers can be extremely helpful in keeping software versions aligned with projects. In Python, it is common to use pip and a requirements.txt file, and in R, the renv package can be used to keep package versions stable within individual projects.","title":"Software Management"},{"location":"06_reproducibility_i/#automation","text":"In the process of making your work more repeatable, you will often be trying to reduce the amount of work you're doing \"by hand\". Reducing the human input necessary at each step of a project is a key to reliably reproducing the same results, but it can also help save you a lot of time in the long run. Have you ever manually edited a figure for a manuscript, only to be asked to change something that negated all your manual edits? Well, in the short run, it may have been quicker to just tinker with the graph by hand, but in the long run, figuring out how to use code to generate the whole thing would have saved you time. Automating tasks often comes with an up-front cost, but it is important for the eventual reproducibility of the work, and will often save you time in the short run. Automation also tends to make tasks scale more easily (editing 10 rows of data by hand is fine, editing 10,000 is much harder), adapt to new scenarios, and extend to future projects. Discussion Question What are some tasks you have automated or want to automate? Have you ever successfully automated a task? Found a way to make something scale or take less time? What was the task, and how did you do it? Are there any things you wish you could automate? What are some barriers to automating them? While we often think about writing scripts to clean data, run analyses, and generate figures, there are even more parts of a research project that can be automated. Here are a few examples: data validation model checking/validation software installation report/manuscript generation citation management email/GitHub/Slack notifications workflow itself (using things like make, Snakemake, Nextflow, targets) Code can be thought of as a set of machine-actionable instructions, or instructions that we write for a computer to follow. What other sets of instructions do you have, either written down or in your head? How can you turn them into something machine-actionable?","title":"Automation"},{"location":"06_reproducibility_i/#disposability","text":"A great approach to repeatability/reproducbility is to ask \"could I generate my results if I lost X?\" What might happen to your work if: you changed some code and your script broke? you couldn't find a figure when a journal asked for it? some software got uninstalled from your computer? your laptop got stolen? some software or computing provider stopped being maintained?","title":"Disposability"},{"location":"06_reproducibility_i/#get-off-your-own-machine","text":"More and more work is being done somewhere other than a personal computer. This could be an HPC cluster at a university or a cloud computing provider. \"Cloud\" just means somebody else is handling the computers, and you get to use them when you need to, typically for a price. Non-local computing resources have varying levels of complexity, flexibility, cost, and scale. Some services like Binder, Colab, and VICE try to abstract more of the computational details away, letting you focus on your code (ideally). Others, like Gitpod, Codespaces, or GitHub Actions, have more limited uses (in a good way).","title":"Get off your own machine"},{"location":"06_reproducibility_i/#dependency-hell","text":"Think for a moment about all the branching possibilities for how a computer could be set up: hardware: CPUs, GPUs, RAM Operating system: many flavors of Linux, MacOS, Windows Software versions: R, Python, etc. Package versions: specific R or Python packages, etc. Simply trying to get the same setup as anyone else is difficult enough, but you can also run into all sorts of dependencies. Let's say you try to update a package to match the version someone else used for a project. However, after updating it, you realize you need to update 3 other packages. After that, you realize you need a newer version of R. You finally manage to get everything set up, but when you go back to a different project the next week, nothing works! All those updates made your code for your other project break. You spend a week fixing your code to work with the newer software, and you're finally done... but now your advisor gives you a dataset 10x the size and says you'll need to run it on the cloud. You throw your laptop out the window and move to the woods to live the life of a hermit. All jokes aside, dealing with software dependencies can be extremely frustrating, and so can setting stuff up on a remote location. It can be even more frustrating if you're trying to reproduce results but you don't actually know the entire software stack used to generate them. There is a way to handle all of these frustrations at once:","title":"Dependency Hell"},{"location":"06_reproducibility_i/#containers","text":"Ok, to be fair, working with containers will also be frustrating. But the beautiful thing about working with containers is that you can handle all of the hard stuff at the start of a project, and you won't have to worry about things changing later on. What are containers? Containers are reproducible computing environments that contain an operating system (OS), software, and even code needed to run analyses. Containers are similar to virtual machines (VMs), but are smaller and easier to share. A big distinction between Containers and VMs is what is within each environment: VMs require the OS to be present within the image, whilst containers rely solemnly on the host OS (and the container engine). Source: Microsoft Cloudblogs A popular container platform is Docker ( wikipedia , \"what is a Docker container?\" ), hosting user created containers on DockerHub , and providing a cross-OS user-friendly toolset for container creation and deployment. RStudio has a number of available Docker containers , each for different use cases and maintained by the Rocker Project . Apptainer (formerly, Singularity), is another popular container engine, which allows you to deploy containers on HPC clusters.","title":"Containers"},{"location":"06_reproducibility_i/#reproducibility-tutorial-i-setting-up-your-project","text":"This section is going to cover a short tutorial spanning this and the next reproducibility sessions. In this tutorial you're going to use software and tools discussed today and previously in FOSS. What you'll be using: GitHub (already installed) Docker (already installed) Snakemake Conda JetStream2","title":"Reproducibility tutorial I: Setting up your project"},{"location":"06_reproducibility_i/#connecting-to-jetstream2","text":"JetStream2 is a service that allows users to create and connect to Virtual Machines (VMs). We have created a number of virtual machines for you to connect to and follow this tutorial, however, you're welcome to use your own machine for this. Warning For security reasons, the google sheets with the IP addresses and passwords is shared solemnly through the HackMD. In order to connect to JetStream2, you will be using the SSH (Secure Shell) protocol, which allows connections over a network. To connect to a machine, open a termial and enter your user number and IP as follows and press enter: ssh <user>@<IP> When asked for the password, copy and paste the corresponding password from the google sheets and press enter. Note : you will NOT see the password being pasted (but it's there!)","title":"Connecting to JetStream2"},{"location":"06_reproducibility_i/#install-conda","text":"Conda is a popular tool for installing software. Typically software you want to use requires other software (dependancies) to be installed. Conda can manage all of this for you. Each available Conda package is part of a \u201crecipe\u201d that includes everything you need to run your software. There are different versions of Conda, including some specific for bioinformatics like Bioconda . We will install Conda and then use it to install some of the tools we need. We will install a lightweight version of Conda called MiniConda . # Download conda and add right permissions wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh chmod +x Miniconda3-py39_4.12.0-Linux-x86_64.sh # install conda silenty (-b) and update (-u) and initial conda run ./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -u ~/miniconda3/bin/conda init # Restart bash so that conda is activated source ~/.bashrc You'll be able to tell when conda is active when next (base) is present next to the to the shell prompt such as (base) user@machine Conda should now be installed and can be used to install other necessary packages! With this in mind, we are going to create our own environment (select y when prompted). conda create --name myenv Activate your new environment with conda activate myenv You can see the list of environments you can activate by doing conda env list","title":"Install Conda"},{"location":"06_reproducibility_i/#installing-packages-through-conda","text":"Conda makes installing packages simple. Due to it's widespread use, conda has a large number of widely available packages; You can search for these in at https://anaconda.org/ . We are going to to use conda to install snakemake. # See what snakemake version are available conda search -c bioconda snakemake # Let's choose the latest, in this case 7.16.0 conda install -c bioconda -c conda-forge -y snakemake=7.16.0 conda will proceed and install all the dependencies required by snakemake # verify the installation snakemake --version You can view the installed conda packages by doing conda list In order to make your environment reproducible, conda allows you to export your environment. conda env export > myenv.yml","title":"Installing packages through conda"},{"location":"06_reproducibility_i/#github-repository-setup-and-documentation","text":"Create a repository on GitHub to document your work: On GitHub , navigate to your account page and create a new repository (add a README to create structure!) Clone your repository to the VM with <repository_url>.git (find the url under the green Code button) Add your history to the README file with history >> README.md and add meaninigful comments. .md is the MarkDown extension that is used for formatting in GitHub (and HackMD!); Read more about here . A well documented document may look similar to: # reproducibility-tutorial ## Computer setup ### Download conda and add right permissions wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh chmod +x Miniconda3-py39_4.12.0-Linux-x86_64.sh ### install conda silenty (-b), update (-u) and initial start ./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -u ~/miniconda3/bin/conda init ### Restart bash so that conda is activated source ~/.bashrc ### See what snakemake version are available conda search -c bioconda snakemake ### Let's choose the 7.16.0 conda install -c bioconda -c conda-forge -y snakemake=7.16.0 ### verify the installation snakemake --version ### Exported conda environment conda env export > myenv.yml ### Cloned git repository git clone https://github.com/<user>/<repository>.git Add, commit and push your changes git add . git commit -m \"adding initial documentation\" git push Github will ask for you username and password; When asked about the password, input a GitHub token. To create a token go to Account > Settings > Developer settings > Personal access tokens > Generate new token , add a note, select all the necessary permissions and select Generate token; Copy the token and use it as password!","title":"GitHub repository setup and documentation"},{"location":"07_reproducibility_ii/","text":"Reproducibility II: Containers \u00b6 Learning Objectives After this lesson, you should be able to: Explain what containers are used for in reproducible research contexts Search for and run a Docker contaienr locally or on a remote system Understand how Version Controlled Software and public data can be used in a container Container Camp: an introdution to Docker containers Introduction to Docker Alt: The Carpentries Introductory Container workshop The Carpentries have an incubator workshop on Docker Containers Containers in Research Workflows Self Assessment \u00b6 A Docker container with the tagname latest ensures old code and data will work on a new computer setup? Failure Never use the latest tag for a publication or archival. The latest version is always being updated and should be considered \"cutting edge\". latest is the default tag name of all Docker images latest versions MAY have backward compatibility with older code and data, but this is not always a given When are containers the right solution? Success Containers are valuable when you need to run an analyses on a remote platform, and you need it to work every time. Failure You need to do some simple text file editing You need to run a web service Reproducibility tutorial II: Using snakemake and containers for your workflow \u00b6 This is a continuation of the reproducibility tutorial started on week 6 . We restart today by reconnecting to your VMs (remember, you can do this on your own machine!). The google sheets with IDs and Passwords is the same as the one shared in the week 6 HackMD. ssh <user>@<IP> Reactivate your environment (if necessary) conda activate myenv It will show it being activated when your shell prompt is (myenv) user@machine Using Docker \u00b6 In this specific tutorial, Docker is already installed in the VM. You can test its installation by doing docker --version If you're using your computer, you could use conda to install docker: conda install -c conda-forge docker However visiting the official docker installation documentation is suggested. For this tutorial, we are going to use specific docker containers. Containers are \"in-use\" docker images; to make things quicker, we can pull these images and make them available. # Docker pull command, downloading the images to your machine docker pull ncbi/sra-tools:latest docker pull biocontainers/fastp:v0.19.6dfsg-1-deb_cv1 docker pull biocontainers/spades:v3.13.0dfsg2-2-deb_cv1 docker pull quay.io/biocontainers/quast:5.0.2--py27pl526ha92aebf_0 # Checking the docker images on the machine docker images # You can run the containers to see how each runs docker run <image name> Using snakemake \u00b6 Snakemake uses metaphors like rules and recipes to describe how a workflow is organized. A rule states what output should be created and defines what steps (the recipe) needs to be follow to create that output. We will import an SRA file, so the first thing we do is create a rule that states what output we should end up with. In our tutorial, we will be working from a single set of reads from the Plasmodium data. We have to build on this by specifying some shell command that will create this. We will use the docker container from the docker image we previously pulled: Tip Use a text editor like nano or vim to create the snakemake file (Snakefile). rule import_from_sra: output: \"learn-snake/sra_files/SRR8245081/SRR8245081.sra\" shell: # Use a docker run command and mount a director \"docker run -v /home/$USER/learn-snake/:/experiment/ \" # set a working directory - this will be created if needed \"--workdir /experiment/sra_files \" # set the docker container \"ncbi/sra-tools:latest\" # use the prefetch tool \"prefetch \" # show progress \"-p 1 \" # get our desired accession \"SRR8245081 \" # output to our desired directory \"--output-directory /experiment/sra_files/ || true\" You can then run the above code with # snakemake calls on the -s option (snakemake file) and -c option (number of cores) snakemake -s Snakefile -c all Known issues Since the conception of this tutorial, an issue with docker writing permissions has arisen when running docker through Snakemake. Running the above script will end with an error, however the files are written and created as necessary. You can read about error 13 here , and the mapping docker volumes issue here if interested. Understanding snakemake As previously commented, snakemake runs through statemets. Each statement is summarized in a set of rules (equivalent to jobs). Each rule is run subsequently after another finishes, with the output of one job being used as the input of the next; Snakemake uses inputs and outputs as \"checkpoints\" between jobs. Inputs and outputs are defined for every rule (but not strictly necessary); Users can also define parameters (which are used within the rule) as well as defining the shell command that will be executed. In the above example, an input isn't defined, but an output is, alongside the shell command (broken down for readability and understandability of the action). Ultimately, when using snakemake you have to know what you want before you start , which is a challenge to new snakemake users. Read more on Snakefiles and rules and snakemake options . A full snakemake pipeline \u00b6 This is what a full snakemake pipeline would look like. Warning This pipeline will break due to the issues highlighted beforehand. We will re-visit this pipeline once the issues will be adderessed, or use different pipeline management tools such as nextflow . Tip Change <USER> to your username! you can find out your username through the command whoami or by looking at the username on the google sheets. #SRA definitions ACCESSION=[\"SRR8245081\",] READS=[\"_1\",\"_2\"] REFERENCE_URLS=[\"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/765/GCF_000002765.4_ASM276v2/GCF_000002765.4_ASM276v2_genomic.fna.gz\", \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/765/GCF_000002765.4_ASM276v2/GCF_000002765.4_ASM276v2_genomic.gff.gz\"] REFERENCES_BASE=[\"GCF_000002765.4_ASM276v2_genomic\",] #containers SRA_TOOLS_CONTAINER=\"ncbi/sra-tools:latest\" FASTP_CONTAINER=\"biocontainers/fastp:v0.19.6dfsg-1-deb_cv1\" SPADES_CONTAINER=\"biocontainers/spades:v3.13.0dfsg2-2-deb_cv1\" QUAST_CONTAINER=\"quay.io/biocontainers/quast:5.0.2--py27pl526ha92aebf_0\" rule all: input: sra=expand(\"/home/<USER>/reproducibility-tutorial/experiment/sra_files/{accession}/{accession}.sra\", accession=ACCESSION), fastq=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}.fastq\", accession=ACCESSION, reads=READS), fastq_trimmed=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}_trimmed.fastq\", accession=ACCESSION, reads=READS), fastp_report=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}_fastp_report.html\", accession=ACCESSION), assembly_contigs=expand(\"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly/contigs.fasta\", accession=ACCESSION), reference_genome=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_fna}.fna\", reference_genome_fna=REFERENCES_BASE), reference_annotation=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_gff}.gff\", reference_genome_gff=REFERENCES_BASE), quast_assembly_report=expand(\"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly_stats/report.html\", accession=ACCESSION) rule SRA_Import: output: \"/home/<USER>/reproducibility-tutorial/experiment/sra_files/{accession}/{accession}.sra\" params: container={SRA_TOOLS_CONTAINER}, command=\"prefetch\", user=\"root\", progress=1, outputdirectory=\"/experiment/sra_files\" shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment \" \"{params.container} {params.command} \" \"-p {params.progress} \" \"--output-directory {params.outputdirectory} \" \"{ACCESSION}\" rule SRA_to_fastq: input: sra=expand(\"/home/<USER>/reproducibility-tutorial/experiment/sra_files/{accession}/{accession}.sra\", accession=ACCESSION) output: \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_1.fastq\", \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_2.fastq\" params: container={SRA_TOOLS_CONTAINER}, command=\"fasterq-dump\", user=\"root\", outputdirectory=\"/experiment/fastq_files\" threads: 8 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment/sra_files/{ACCESSION}/ \" \"{params.container} {params.command} \" \"{ACCESSION}.sra \" \"-O {params.outputdirectory} \" rule fastq_qc: input: fastq=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}.fastq\", accession=ACCESSION, reads=READS) output: \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_1_trimmed.fastq\", \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_2_trimmed.fastq\", \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}_fastp_report.html\" params: container={FASTP_CONTAINER}, command=\"fastp\", user=\"root\", outputdirectory=\"/experiment/fastq_files\" threads: 2 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment/fastq_files/ \" \"{params.container} {params.command} \" \"-V \" \"--cut_front \" \"--cut_front_mean_quality 30 \" \"- R '{ACCESSION} fastp report' \" \"--html {ACCESSION}_fastp_report.html \" \"-i /experiment/fastq_files/{ACCESSION}.sra_1.fastq \" \"-o /experiment/fastq_files/{ACCESSION}.sra_1_trimmed.fastq \" \"-I /experiment/fastq_files/{ACCESSION}.sra_2.fastq \" \"-O /experiment/fastq_files/{ACCESSION}.sra_2_trimmed.fastq\" rule SPAdes_assembly: input: fastq_trimmed=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}_trimmed.fastq\", accession=ACCESSION, reads=READS) output: \"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly/contigs.fasta\" params: container={SPADES_CONTAINER}, command=\"spades.py\", user=\"root\", outputdirectory=\"/experiment/{accession}_assembly\" threads: 2 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment/fastq_files/ \" \"{params.container} {params.command} \" \"-1 /experiment/fastq_files/{ACCESSION}.sra_1_trimmed.fastq \" \"-2 /experiment/fastq_files/{ACCESSION}.sra_2_trimmed.fastq \" \"-o {params.outputdirectory}\" rule import_reference_genome: output: reference_genome=\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{REFERENCES_BASE}.fna\", reference_annotation=\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{REFERENCES_BASE}.gff\" shell: \"mkdir -p /home/<USER>/reproducibility-tutorial/experiment/reference_genome && \" \"cd /home/<USER>/reproducibility-tutorial/experiment/reference_genome && \" \"wget {REFERENCE_URLS} && \" \"gzip -d {REFERENCES_BASE}*.gz\" rule assembly_stats: input: assembly_contigs=expand(\"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly/contigs.fasta\", accession=ACCESSION), reference_genome=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_fna}.fna\", reference_genome_fna=REFERENCES_BASE), reference_annotation=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_gff}.gff\", reference_genome_gff=REFERENCES_BASE) output: \"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly_stats/report.html\" params: container={QUAST_CONTAINER}, command=\"quast.py\", user=\"root\", outputdirectory=\"/experiment/{accession}_assembly_stats\", container_input=expand(\"/experiment/{accession}_assembly/contigs.fasta\", accession=ACCESSION) threads: 2 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /home/<USER>/reproducibility-tutorial/experiment/reference_genome/ \" \"{params.container} {params.command} \" \"-o {params.outputdirectory} \" \"--features gene:/experiment/reference_genome/{REFERENCES_BASE}.gff \" \"-r /experiment/reference_genome/{REFERENCES_BASE}.fna \" \"-t 8 \" \"--eukaryote \" \"--circos \" \"{params.container_input}\" Goals of the above pipeline If you are new to the world of genomics, the above pipeline is supposed to: Download data from the SRA database Extract genetic information ( fastq format) Carry out quality control for the DNA strands Assemble genome Download a reference genome Calculate statistics between the assembly and the reference Want a challenge? Still want to run the pipeline? As a challenge, you can infer and execute each docker command individually. For example, the first docker command is docker run --user root -v ~/reproducibility-tutorial/experiment:/experiment --workdir /experiment ncbi/sra-tools:latest prefetch -p 1 --output-directory /experiment/sra_files SRR8245081 Document your work \u00b6 As previously mentioned, you should copy your results to Github and commit! Document the steps you take/took, snakemake statements, errors you run into and possible resolutions.","title":"Reproducibility II: Containers"},{"location":"07_reproducibility_ii/#reproducibility-ii-containers","text":"Learning Objectives After this lesson, you should be able to: Explain what containers are used for in reproducible research contexts Search for and run a Docker contaienr locally or on a remote system Understand how Version Controlled Software and public data can be used in a container Container Camp: an introdution to Docker containers Introduction to Docker Alt: The Carpentries Introductory Container workshop The Carpentries have an incubator workshop on Docker Containers Containers in Research Workflows","title":"Reproducibility II: Containers"},{"location":"07_reproducibility_ii/#self-assessment","text":"A Docker container with the tagname latest ensures old code and data will work on a new computer setup? Failure Never use the latest tag for a publication or archival. The latest version is always being updated and should be considered \"cutting edge\". latest is the default tag name of all Docker images latest versions MAY have backward compatibility with older code and data, but this is not always a given When are containers the right solution? Success Containers are valuable when you need to run an analyses on a remote platform, and you need it to work every time. Failure You need to do some simple text file editing You need to run a web service","title":"Self Assessment"},{"location":"07_reproducibility_ii/#reproducibility-tutorial-ii-using-snakemake-and-containers-for-your-workflow","text":"This is a continuation of the reproducibility tutorial started on week 6 . We restart today by reconnecting to your VMs (remember, you can do this on your own machine!). The google sheets with IDs and Passwords is the same as the one shared in the week 6 HackMD. ssh <user>@<IP> Reactivate your environment (if necessary) conda activate myenv It will show it being activated when your shell prompt is (myenv) user@machine","title":"Reproducibility tutorial II: Using snakemake and containers for your workflow"},{"location":"07_reproducibility_ii/#using-docker","text":"In this specific tutorial, Docker is already installed in the VM. You can test its installation by doing docker --version If you're using your computer, you could use conda to install docker: conda install -c conda-forge docker However visiting the official docker installation documentation is suggested. For this tutorial, we are going to use specific docker containers. Containers are \"in-use\" docker images; to make things quicker, we can pull these images and make them available. # Docker pull command, downloading the images to your machine docker pull ncbi/sra-tools:latest docker pull biocontainers/fastp:v0.19.6dfsg-1-deb_cv1 docker pull biocontainers/spades:v3.13.0dfsg2-2-deb_cv1 docker pull quay.io/biocontainers/quast:5.0.2--py27pl526ha92aebf_0 # Checking the docker images on the machine docker images # You can run the containers to see how each runs docker run <image name>","title":"Using Docker"},{"location":"07_reproducibility_ii/#using-snakemake","text":"Snakemake uses metaphors like rules and recipes to describe how a workflow is organized. A rule states what output should be created and defines what steps (the recipe) needs to be follow to create that output. We will import an SRA file, so the first thing we do is create a rule that states what output we should end up with. In our tutorial, we will be working from a single set of reads from the Plasmodium data. We have to build on this by specifying some shell command that will create this. We will use the docker container from the docker image we previously pulled: Tip Use a text editor like nano or vim to create the snakemake file (Snakefile). rule import_from_sra: output: \"learn-snake/sra_files/SRR8245081/SRR8245081.sra\" shell: # Use a docker run command and mount a director \"docker run -v /home/$USER/learn-snake/:/experiment/ \" # set a working directory - this will be created if needed \"--workdir /experiment/sra_files \" # set the docker container \"ncbi/sra-tools:latest\" # use the prefetch tool \"prefetch \" # show progress \"-p 1 \" # get our desired accession \"SRR8245081 \" # output to our desired directory \"--output-directory /experiment/sra_files/ || true\" You can then run the above code with # snakemake calls on the -s option (snakemake file) and -c option (number of cores) snakemake -s Snakefile -c all Known issues Since the conception of this tutorial, an issue with docker writing permissions has arisen when running docker through Snakemake. Running the above script will end with an error, however the files are written and created as necessary. You can read about error 13 here , and the mapping docker volumes issue here if interested. Understanding snakemake As previously commented, snakemake runs through statemets. Each statement is summarized in a set of rules (equivalent to jobs). Each rule is run subsequently after another finishes, with the output of one job being used as the input of the next; Snakemake uses inputs and outputs as \"checkpoints\" between jobs. Inputs and outputs are defined for every rule (but not strictly necessary); Users can also define parameters (which are used within the rule) as well as defining the shell command that will be executed. In the above example, an input isn't defined, but an output is, alongside the shell command (broken down for readability and understandability of the action). Ultimately, when using snakemake you have to know what you want before you start , which is a challenge to new snakemake users. Read more on Snakefiles and rules and snakemake options .","title":"Using snakemake"},{"location":"07_reproducibility_ii/#a-full-snakemake-pipeline","text":"This is what a full snakemake pipeline would look like. Warning This pipeline will break due to the issues highlighted beforehand. We will re-visit this pipeline once the issues will be adderessed, or use different pipeline management tools such as nextflow . Tip Change <USER> to your username! you can find out your username through the command whoami or by looking at the username on the google sheets. #SRA definitions ACCESSION=[\"SRR8245081\",] READS=[\"_1\",\"_2\"] REFERENCE_URLS=[\"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/765/GCF_000002765.4_ASM276v2/GCF_000002765.4_ASM276v2_genomic.fna.gz\", \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/765/GCF_000002765.4_ASM276v2/GCF_000002765.4_ASM276v2_genomic.gff.gz\"] REFERENCES_BASE=[\"GCF_000002765.4_ASM276v2_genomic\",] #containers SRA_TOOLS_CONTAINER=\"ncbi/sra-tools:latest\" FASTP_CONTAINER=\"biocontainers/fastp:v0.19.6dfsg-1-deb_cv1\" SPADES_CONTAINER=\"biocontainers/spades:v3.13.0dfsg2-2-deb_cv1\" QUAST_CONTAINER=\"quay.io/biocontainers/quast:5.0.2--py27pl526ha92aebf_0\" rule all: input: sra=expand(\"/home/<USER>/reproducibility-tutorial/experiment/sra_files/{accession}/{accession}.sra\", accession=ACCESSION), fastq=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}.fastq\", accession=ACCESSION, reads=READS), fastq_trimmed=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}_trimmed.fastq\", accession=ACCESSION, reads=READS), fastp_report=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}_fastp_report.html\", accession=ACCESSION), assembly_contigs=expand(\"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly/contigs.fasta\", accession=ACCESSION), reference_genome=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_fna}.fna\", reference_genome_fna=REFERENCES_BASE), reference_annotation=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_gff}.gff\", reference_genome_gff=REFERENCES_BASE), quast_assembly_report=expand(\"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly_stats/report.html\", accession=ACCESSION) rule SRA_Import: output: \"/home/<USER>/reproducibility-tutorial/experiment/sra_files/{accession}/{accession}.sra\" params: container={SRA_TOOLS_CONTAINER}, command=\"prefetch\", user=\"root\", progress=1, outputdirectory=\"/experiment/sra_files\" shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment \" \"{params.container} {params.command} \" \"-p {params.progress} \" \"--output-directory {params.outputdirectory} \" \"{ACCESSION}\" rule SRA_to_fastq: input: sra=expand(\"/home/<USER>/reproducibility-tutorial/experiment/sra_files/{accession}/{accession}.sra\", accession=ACCESSION) output: \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_1.fastq\", \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_2.fastq\" params: container={SRA_TOOLS_CONTAINER}, command=\"fasterq-dump\", user=\"root\", outputdirectory=\"/experiment/fastq_files\" threads: 8 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment/sra_files/{ACCESSION}/ \" \"{params.container} {params.command} \" \"{ACCESSION}.sra \" \"-O {params.outputdirectory} \" rule fastq_qc: input: fastq=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}.fastq\", accession=ACCESSION, reads=READS) output: \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_1_trimmed.fastq\", \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra_2_trimmed.fastq\", \"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}_fastp_report.html\" params: container={FASTP_CONTAINER}, command=\"fastp\", user=\"root\", outputdirectory=\"/experiment/fastq_files\" threads: 2 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment/fastq_files/ \" \"{params.container} {params.command} \" \"-V \" \"--cut_front \" \"--cut_front_mean_quality 30 \" \"- R '{ACCESSION} fastp report' \" \"--html {ACCESSION}_fastp_report.html \" \"-i /experiment/fastq_files/{ACCESSION}.sra_1.fastq \" \"-o /experiment/fastq_files/{ACCESSION}.sra_1_trimmed.fastq \" \"-I /experiment/fastq_files/{ACCESSION}.sra_2.fastq \" \"-O /experiment/fastq_files/{ACCESSION}.sra_2_trimmed.fastq\" rule SPAdes_assembly: input: fastq_trimmed=expand(\"/home/<USER>/reproducibility-tutorial/experiment/fastq_files/{accession}.sra{reads}_trimmed.fastq\", accession=ACCESSION, reads=READS) output: \"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly/contigs.fasta\" params: container={SPADES_CONTAINER}, command=\"spades.py\", user=\"root\", outputdirectory=\"/experiment/{accession}_assembly\" threads: 2 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /experiment/fastq_files/ \" \"{params.container} {params.command} \" \"-1 /experiment/fastq_files/{ACCESSION}.sra_1_trimmed.fastq \" \"-2 /experiment/fastq_files/{ACCESSION}.sra_2_trimmed.fastq \" \"-o {params.outputdirectory}\" rule import_reference_genome: output: reference_genome=\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{REFERENCES_BASE}.fna\", reference_annotation=\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{REFERENCES_BASE}.gff\" shell: \"mkdir -p /home/<USER>/reproducibility-tutorial/experiment/reference_genome && \" \"cd /home/<USER>/reproducibility-tutorial/experiment/reference_genome && \" \"wget {REFERENCE_URLS} && \" \"gzip -d {REFERENCES_BASE}*.gz\" rule assembly_stats: input: assembly_contigs=expand(\"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly/contigs.fasta\", accession=ACCESSION), reference_genome=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_fna}.fna\", reference_genome_fna=REFERENCES_BASE), reference_annotation=expand(\"/home/<USER>/reproducibility-tutorial/experiment/reference_genome/{reference_genome_gff}.gff\", reference_genome_gff=REFERENCES_BASE) output: \"/home/<USER>/reproducibility-tutorial/experiment/{accession}_assembly_stats/report.html\" params: container={QUAST_CONTAINER}, command=\"quast.py\", user=\"root\", outputdirectory=\"/experiment/{accession}_assembly_stats\", container_input=expand(\"/experiment/{accession}_assembly/contigs.fasta\", accession=ACCESSION) threads: 2 shell: \"docker run --user {params.user} \" \" -v ~/reproducibility-tutorial/experiment:/experiment \" \"--workdir /home/<USER>/reproducibility-tutorial/experiment/reference_genome/ \" \"{params.container} {params.command} \" \"-o {params.outputdirectory} \" \"--features gene:/experiment/reference_genome/{REFERENCES_BASE}.gff \" \"-r /experiment/reference_genome/{REFERENCES_BASE}.fna \" \"-t 8 \" \"--eukaryote \" \"--circos \" \"{params.container_input}\" Goals of the above pipeline If you are new to the world of genomics, the above pipeline is supposed to: Download data from the SRA database Extract genetic information ( fastq format) Carry out quality control for the DNA strands Assemble genome Download a reference genome Calculate statistics between the assembly and the reference Want a challenge? Still want to run the pipeline? As a challenge, you can infer and execute each docker command individually. For example, the first docker command is docker run --user root -v ~/reproducibility-tutorial/experiment:/experiment --workdir /experiment ncbi/sra-tools:latest prefetch -p 1 --output-directory /experiment/sra_files SRR8245081","title":"A full snakemake pipeline"},{"location":"07_reproducibility_ii/#document-your-work","text":"As previously mentioned, you should copy your results to Github and commit! Document the steps you take/took, snakemake statements, errors you run into and possible resolutions.","title":"Document your work"},{"location":"code_of_conduct/","text":"Code of Conduct \u00b6 In conjunction with for using CyVerse cyberinfrastructure, this Code of Conduct applies to all Event participants and their activities while using CyVerse resources and/or attending the Event. CyVerse is dedicated to providing professional computational research and educational experiences for all of our users, regardless of domain focus, academic status, educational level, gender/gender identity/expression, age, sexual orientation, mental or physical ability, physical appearance, body size, race, ethnicity, religion (or lack thereof), technology choices, dietary preferences, or any other personal characteristic. When using CyVerse or participating at an Event, we expect you to: Interact with others and use CyVerse professionally and ethically by complying with our Policies. Constructively critize ideas and processes, not people. Follow the Golden Rule (treat others as you want to be treated) when interacting online or in-person with collaborators, trainers, and support staff. Comply with this Code in spirit as much as the letter, as it is neither exhaustive nor complete in identifying any and all possible unacceptable conduct. We do not tolerate harassment of other users or staff in any form (including, but not limited to, violent threats or language, derogatory language or jokes, doxing, insults, advocating for or encouraging any of these behaviors). Sexual language and imagery are not appropriate at any time (excludes Protected Health Information in compliance with HIPAA). Any user violating this Code may be expelled from the platform and the workshop at CyVerse's sole discretion without warning. To report a violation of this Code, directly message a trainer via Slack or email info@cyverse.org with the following information: Your contact information Names (real, username, pseudonyms) of any individuals involved, and or witness(es) if any. Your account of what occurred and if the incident is ongoing. If there is a publicly available record (a tweet, public chat log, etc.), please include a link or attachment. Any additional information that may be helpful in resolving the issue.","title":"Code of Conduct"},{"location":"code_of_conduct/#code-of-conduct","text":"In conjunction with for using CyVerse cyberinfrastructure, this Code of Conduct applies to all Event participants and their activities while using CyVerse resources and/or attending the Event. CyVerse is dedicated to providing professional computational research and educational experiences for all of our users, regardless of domain focus, academic status, educational level, gender/gender identity/expression, age, sexual orientation, mental or physical ability, physical appearance, body size, race, ethnicity, religion (or lack thereof), technology choices, dietary preferences, or any other personal characteristic. When using CyVerse or participating at an Event, we expect you to: Interact with others and use CyVerse professionally and ethically by complying with our Policies. Constructively critize ideas and processes, not people. Follow the Golden Rule (treat others as you want to be treated) when interacting online or in-person with collaborators, trainers, and support staff. Comply with this Code in spirit as much as the letter, as it is neither exhaustive nor complete in identifying any and all possible unacceptable conduct. We do not tolerate harassment of other users or staff in any form (including, but not limited to, violent threats or language, derogatory language or jokes, doxing, insults, advocating for or encouraging any of these behaviors). Sexual language and imagery are not appropriate at any time (excludes Protected Health Information in compliance with HIPAA). Any user violating this Code may be expelled from the platform and the workshop at CyVerse's sole discretion without warning. To report a violation of this Code, directly message a trainer via Slack or email info@cyverse.org with the following information: Your contact information Names (real, username, pseudonyms) of any individuals involved, and or witness(es) if any. Your account of what occurred and if the incident is ongoing. If there is a publicly available record (a tweet, public chat log, etc.), please include a link or attachment. Any additional information that may be helpful in resolving the issue.","title":"Code of Conduct"},{"location":"glossary/","text":"Glossary & Acronyms \u00b6 A action: automate a workflow in the context of CI/CD, see GitHub Actions agile: development methodology for organizing a team to complete tasks organized over short periods called 'sprints' allocation: portion of a resource assigned to a particular recipient, typical unit is a core or node hour Anaconda: open source data science platform. Anaconda.com application: also called an 'app', a software designed to help the user to perform specific task awesome: a curated set of lists that provide insight into awesome software projects on GitHub AVU: Attribute-Value-Unit a components for iRODS metadata . B beta: a software version which is not yet ready for publication but is being tested bash: Bash is the GNU Project's shell, the Bourne-Again Shell biocontainer: a community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute bioinformatics packages (e.g conda) and containers (e.g docker, singularity) bioconda: a channel for the conda package manager specializing in bioinformatics software C CLI: the UNIX shell command line interface , most typically BASH command: a set of instructions sent to the computer, typically in a typed interface conda: an installation type of the Anaconda data science platform. Command line application for managing packages and environments container: virtualization of an operating system run within an isolated user space Continuous Integration: (CI) is testing automation to check that the application is not broken whenever new commits are integrated into the main branch Continuous Delivery: (CD) is an extension of 'continuous integration' to make sure that you can release new changes in a sustainable way Continuous Deployment: a step further than 'continuous delivery', every change that passes all stages of your production pipeline is released Continuous Development: a process for iterative software development and is an umbrella over several other processes including 'continuous integration', 'continuous testing', 'continuous delivery' and 'continuous deployment' Continuous Testing: a process of testing and automating software development. CRAN: The Comprehensive R Archive Network CyVerse tool: Software program that is integrated into the back end of the DE for use in DE apps CyVerse app: graphic interface of a tool made available for use in the DE D Debian: a free OS , base of other Linux distributions such as Ubuntu Development: the environment on your computer where you write code DevOps Software *Dev*elopment and information techology *Op*erations techniques for shortening the time to change software in relation to CI/CD Discovery Environment (DE): a data science workbench for running executable, interactive, and high throughput applications in CyVerse DE distribution: abbreviated as 'distro', an operating system made from a software collection based upon the Linux kernel Docker: Docker is an open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools. A program that runs and handles life-cycle of containers and images DockerHub: an official registry of docker containers, operated by Docker. DockerHub DOI: a digital object identifier. A persistant identifier number, managed by the doi.org Dockerfile: a text document that contains all the commands you would normally execute manually in order to build a Docker image. Docker can build images automatically by reading the instructions from a Dockerfile E environment: software that includes operating system, database system, specific tools for analysis entrypoint: In a Dockerfile, an ENTRYPOINT is an optional definition for the first part of the command to be run F FOSS: (1) Free and Open Source Software , (2) Foundational Open Science Skills - this class! function: a named section of a program that performs a specific task G git: a version control system software gitter: a Github based messaging service that uses markdown gitter.im GitHub: a website for hosting git repositories - owned by Microsoft GitHub GitLab: a website for hosting git repositories GitLab GitOps: using git framework as a means of deploying infrastructure on cloud using Kubernetes GPU: graphic processing unit GUI: graphical user interface H hack: a quick job that produces what is needed, but not well HPC: High Performance Computer, for large syncronous computation HTC: High Throughput Computer, for many parallel tasks I IaaS: Infrastructure as a Service . online services that provide APIs iCommands: command line application for accessing iRODS Data Store IDE: integrated development environment, typically a graphical interface for working with code language or packages instance: a single virtul machine image: self-contained, read-only 'snapshot' of your applications and packages, with all their dependencies iRODS: an open source integrated Rule-Oriented Data Management System, iRODS.org J Java: programming language, class-based, object-oriented JavaScript: programming language JSON: Java Script Object Notation, data interchange format that uses human-readable text Jupyter(Hub,Lab,Notebooks): an IDE, originally the iPythonNotebook, operates in the browser Project Jupyter K kernel: central component of most operating systems (OS) Kubernetes: an open source container orchestration platform created by Google Kubernetes is often referred to as K8s L lib: a UNIX library linux: open source Unix-like operating system M makefile: a file containing a set of directives used by a make build automation tool markdown: a lightweight markup language with plain text formatting syntax metadata:: data about data, useful for searching and querying multi-thread: a process which runs on more than one CPU or GPU core at the same time master node: responsible for deciding what runs on all of the cluster's nodes. Can include scheduling workloads, like containerized applications, and managing the workloads' lifecycle, scaling, and upgrades. The master also manages network and storage resources for those workloads Mac OS X: Apple's popular desktop OS N node: a computer, typically 1 or 2 core (with many threads) server in a cloud or HPC center O ontology: formal naming and structural hierarchy used to describe data, also called a knowledge graph organization: a group, in the context of GitHub a place where developers contribute code to repositories Operating System (OS): software that manages computer hardware, software resources, and provides common services for computer programs Open Science Grid (OSG): national, distributed computing partnership for data-intensive research opensciencegrid.org ORCID: Open Researcher and Contributor ID ( ORCiD ), a persistent digital identifier that distinguishes you from every other researcher P PaaS: Platform as Service run and manage applications in cloud without complexity of developing it yourself package: an app designed for a particular langauge package manager: a collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer's operating system in a consistent manner Production: environment where users access the final code after all of the updates and testing Python: interpreted, high-level, general-purpose programming language Python.org Q QUAY.io: private Docker registry QUAY.io R R: data science programming language R Project recipe file: a file with installation scripts used for building software such as containers, e.g. Dockerfile registry: a storage and content delivery system, such as that used by Docker remote desktop: a VM with a graphic user interface accessed via a browser repo(sitory): a directory structure for hosting code and data RST: ReStructuredText, a markdown type file ReadTheDocs: a web service for rendering documentation (that this website uses) readthedocs.org and readthedocs.com root: the administrative user on a linux kernel - use your powers wisely S SaaS: Software as a Service web based platform for using software schema: a metadata standard for labeling, tagging or coding for recording & cataloging information or structuring descriptive records. see schema.org scrum: daily set of tasks and evalautions as part of a sprint. shell: is a command line interface program that runs other programs (may be complex, technical programs or very simple programs such as making a directory). These simple, stand-alone programs are called commands Singularity: a container software, used widely on HPC, created by SyLabs SLACK: Searchable Log of All Conversation and Knowledge, a team communication tool slack.com sprint: set period of time during which specific work has to be completed and made ready for review Singularity def file: (definition file) recipe for building a Singualrity container Stage: environment that is as similar to the production environment as can be for final testing T tar: software utility for collecting many files into one archive file, often referred to as a tarball tensor: algebraic object that describes a linear mapping from one set of algebraic objects to another terminal: a windowed emulator for directly enterinc commands to a computer thread: a CPU process or a series of linked messages in a discussion board tool: In the context of CyVerse Discovery Environment, a Docker Container TPU: tensor processing unit Travis: Travis-CI , a continuous integration software U Ubuntu: most popular Linux OS distribution , based on Debian UNIX: operating system user: the profile under which applications are started and run, root is the most powerful system administrator V VICE: Visual Interactive Computing Environment - Cyverse Data Science Workbench virtual machine: is a software computer that, like a physical computer, runs an operating system and applications W waterfall: software development broken into linear sequential phases, similar to a Gantt chart webGL: JavaScript API for rendering interactive 2D and 3D graphics within any compatible web browser without the use of plug-ins Windows: Microsoft's most popular desktop OS workspace: (vs. repo) worker node: A cluster typically has one or more nodes, which are the worker machines that run your containerized applications and other workloads. Each node is managed from the master, which receives updates on each node's self-reported status. X XML: Extensible Markup Language, data interchange format that uses human-readable text Y YAML: YAML Ain't Markup Language, data interchange format that uses human-readable text Z ZenHub: team collaboration solution built directly into GitHub that uses kanban style boards Zenodo: general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN zip: a compressed file format zsh: Z-Shell , now the default shell on new Mac OS X","title":"Glossary & Acronyms"},{"location":"glossary/#glossary-acronyms","text":"A action: automate a workflow in the context of CI/CD, see GitHub Actions agile: development methodology for organizing a team to complete tasks organized over short periods called 'sprints' allocation: portion of a resource assigned to a particular recipient, typical unit is a core or node hour Anaconda: open source data science platform. Anaconda.com application: also called an 'app', a software designed to help the user to perform specific task awesome: a curated set of lists that provide insight into awesome software projects on GitHub AVU: Attribute-Value-Unit a components for iRODS metadata . B beta: a software version which is not yet ready for publication but is being tested bash: Bash is the GNU Project's shell, the Bourne-Again Shell biocontainer: a community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute bioinformatics packages (e.g conda) and containers (e.g docker, singularity) bioconda: a channel for the conda package manager specializing in bioinformatics software C CLI: the UNIX shell command line interface , most typically BASH command: a set of instructions sent to the computer, typically in a typed interface conda: an installation type of the Anaconda data science platform. Command line application for managing packages and environments container: virtualization of an operating system run within an isolated user space Continuous Integration: (CI) is testing automation to check that the application is not broken whenever new commits are integrated into the main branch Continuous Delivery: (CD) is an extension of 'continuous integration' to make sure that you can release new changes in a sustainable way Continuous Deployment: a step further than 'continuous delivery', every change that passes all stages of your production pipeline is released Continuous Development: a process for iterative software development and is an umbrella over several other processes including 'continuous integration', 'continuous testing', 'continuous delivery' and 'continuous deployment' Continuous Testing: a process of testing and automating software development. CRAN: The Comprehensive R Archive Network CyVerse tool: Software program that is integrated into the back end of the DE for use in DE apps CyVerse app: graphic interface of a tool made available for use in the DE D Debian: a free OS , base of other Linux distributions such as Ubuntu Development: the environment on your computer where you write code DevOps Software *Dev*elopment and information techology *Op*erations techniques for shortening the time to change software in relation to CI/CD Discovery Environment (DE): a data science workbench for running executable, interactive, and high throughput applications in CyVerse DE distribution: abbreviated as 'distro', an operating system made from a software collection based upon the Linux kernel Docker: Docker is an open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools. A program that runs and handles life-cycle of containers and images DockerHub: an official registry of docker containers, operated by Docker. DockerHub DOI: a digital object identifier. A persistant identifier number, managed by the doi.org Dockerfile: a text document that contains all the commands you would normally execute manually in order to build a Docker image. Docker can build images automatically by reading the instructions from a Dockerfile E environment: software that includes operating system, database system, specific tools for analysis entrypoint: In a Dockerfile, an ENTRYPOINT is an optional definition for the first part of the command to be run F FOSS: (1) Free and Open Source Software , (2) Foundational Open Science Skills - this class! function: a named section of a program that performs a specific task G git: a version control system software gitter: a Github based messaging service that uses markdown gitter.im GitHub: a website for hosting git repositories - owned by Microsoft GitHub GitLab: a website for hosting git repositories GitLab GitOps: using git framework as a means of deploying infrastructure on cloud using Kubernetes GPU: graphic processing unit GUI: graphical user interface H hack: a quick job that produces what is needed, but not well HPC: High Performance Computer, for large syncronous computation HTC: High Throughput Computer, for many parallel tasks I IaaS: Infrastructure as a Service . online services that provide APIs iCommands: command line application for accessing iRODS Data Store IDE: integrated development environment, typically a graphical interface for working with code language or packages instance: a single virtul machine image: self-contained, read-only 'snapshot' of your applications and packages, with all their dependencies iRODS: an open source integrated Rule-Oriented Data Management System, iRODS.org J Java: programming language, class-based, object-oriented JavaScript: programming language JSON: Java Script Object Notation, data interchange format that uses human-readable text Jupyter(Hub,Lab,Notebooks): an IDE, originally the iPythonNotebook, operates in the browser Project Jupyter K kernel: central component of most operating systems (OS) Kubernetes: an open source container orchestration platform created by Google Kubernetes is often referred to as K8s L lib: a UNIX library linux: open source Unix-like operating system M makefile: a file containing a set of directives used by a make build automation tool markdown: a lightweight markup language with plain text formatting syntax metadata:: data about data, useful for searching and querying multi-thread: a process which runs on more than one CPU or GPU core at the same time master node: responsible for deciding what runs on all of the cluster's nodes. Can include scheduling workloads, like containerized applications, and managing the workloads' lifecycle, scaling, and upgrades. The master also manages network and storage resources for those workloads Mac OS X: Apple's popular desktop OS N node: a computer, typically 1 or 2 core (with many threads) server in a cloud or HPC center O ontology: formal naming and structural hierarchy used to describe data, also called a knowledge graph organization: a group, in the context of GitHub a place where developers contribute code to repositories Operating System (OS): software that manages computer hardware, software resources, and provides common services for computer programs Open Science Grid (OSG): national, distributed computing partnership for data-intensive research opensciencegrid.org ORCID: Open Researcher and Contributor ID ( ORCiD ), a persistent digital identifier that distinguishes you from every other researcher P PaaS: Platform as Service run and manage applications in cloud without complexity of developing it yourself package: an app designed for a particular langauge package manager: a collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer's operating system in a consistent manner Production: environment where users access the final code after all of the updates and testing Python: interpreted, high-level, general-purpose programming language Python.org Q QUAY.io: private Docker registry QUAY.io R R: data science programming language R Project recipe file: a file with installation scripts used for building software such as containers, e.g. Dockerfile registry: a storage and content delivery system, such as that used by Docker remote desktop: a VM with a graphic user interface accessed via a browser repo(sitory): a directory structure for hosting code and data RST: ReStructuredText, a markdown type file ReadTheDocs: a web service for rendering documentation (that this website uses) readthedocs.org and readthedocs.com root: the administrative user on a linux kernel - use your powers wisely S SaaS: Software as a Service web based platform for using software schema: a metadata standard for labeling, tagging or coding for recording & cataloging information or structuring descriptive records. see schema.org scrum: daily set of tasks and evalautions as part of a sprint. shell: is a command line interface program that runs other programs (may be complex, technical programs or very simple programs such as making a directory). These simple, stand-alone programs are called commands Singularity: a container software, used widely on HPC, created by SyLabs SLACK: Searchable Log of All Conversation and Knowledge, a team communication tool slack.com sprint: set period of time during which specific work has to be completed and made ready for review Singularity def file: (definition file) recipe for building a Singualrity container Stage: environment that is as similar to the production environment as can be for final testing T tar: software utility for collecting many files into one archive file, often referred to as a tarball tensor: algebraic object that describes a linear mapping from one set of algebraic objects to another terminal: a windowed emulator for directly enterinc commands to a computer thread: a CPU process or a series of linked messages in a discussion board tool: In the context of CyVerse Discovery Environment, a Docker Container TPU: tensor processing unit Travis: Travis-CI , a continuous integration software U Ubuntu: most popular Linux OS distribution , based on Debian UNIX: operating system user: the profile under which applications are started and run, root is the most powerful system administrator V VICE: Visual Interactive Computing Environment - Cyverse Data Science Workbench virtual machine: is a software computer that, like a physical computer, runs an operating system and applications W waterfall: software development broken into linear sequential phases, similar to a Gantt chart webGL: JavaScript API for rendering interactive 2D and 3D graphics within any compatible web browser without the use of plug-ins Windows: Microsoft's most popular desktop OS workspace: (vs. repo) worker node: A cluster typically has one or more nodes, which are the worker machines that run your containerized applications and other workloads. Each node is managed from the master, which receives updates on each node's self-reported status. X XML: Extensible Markup Language, data interchange format that uses human-readable text Y YAML: YAML Ain't Markup Language, data interchange format that uses human-readable text Z ZenHub: team collaboration solution built directly into GitHub that uses kanban style boards Zenodo: general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN zip: a compressed file format zsh: Z-Shell , now the default shell on new Mac OS X","title":"Glossary &amp; Acronyms"},{"location":"installation/","text":"Pre-FOSS Setup \u00b6 Welcome to FOSS Online, we're happy you're here! To get you ready to hit the ground running, please set up the prerequisite accounts and software listed below before the course starts. Account Creation \u00b6 We will be using several services that require you to create a user account. Account Notes CyVerse Register for a CyVerse account by following the instructions here . GitHub GitHub will be used to store lecture materials and your own work. We will use GitHub Education and its free features for hands-on. Docker Link your GitHub account to the DockerHub. Slack We use the Slack cyverselearning organization, you should have received an invitation via email. You can use Slack in the browser, but the desktop app is usually less buggy. HackMD We will use HackMD in order to facilitate daily discussions, questions and general notes. Link your HackMD using your GitHub account Link to HackMD https://hackmd.io/@cyverse-foss/r1AIPa1Zi/edit Dual Monitors vs Side-by-Side We strongly recommend you have dual monitors set-up while attending virtual FOSS Zoom lessons. We will be doing a lot of screen-sharing, and this will make your own interactive sessions less visible, or you will have to make them less than full screen. If you only have one monitor, make sure to exit full screen mode on Zoom and your browser, so you can view everything side-by-side Required Software \u00b6 You will need to have the following software installed on your personal computer: Software Notes Web Browser Chrome or Firefox . Text Editor VS Code or SublimeText Attention Windows users Much of what we are going to be teaching is based on open-source software which operates on cloud and is incompatible with Windows OS. Unix-based systems such as Linux Ubuntu and MacOS X , as many scientific tools require a Unix Operating System (OS). There are a number of software that allow Windows users to execute Unix commands, however we recommend the use of Windows Subsystem for Linux (WSL) 2.0 . VS Code is a Microsoft product and integrates seamlessly with Unix systems, we therefore strongly encourage you to install Code on your Windows OS.","title":"Installation"},{"location":"installation/#pre-foss-setup","text":"Welcome to FOSS Online, we're happy you're here! To get you ready to hit the ground running, please set up the prerequisite accounts and software listed below before the course starts.","title":"Pre-FOSS Setup"},{"location":"installation/#account-creation","text":"We will be using several services that require you to create a user account. Account Notes CyVerse Register for a CyVerse account by following the instructions here . GitHub GitHub will be used to store lecture materials and your own work. We will use GitHub Education and its free features for hands-on. Docker Link your GitHub account to the DockerHub. Slack We use the Slack cyverselearning organization, you should have received an invitation via email. You can use Slack in the browser, but the desktop app is usually less buggy. HackMD We will use HackMD in order to facilitate daily discussions, questions and general notes. Link your HackMD using your GitHub account Link to HackMD https://hackmd.io/@cyverse-foss/r1AIPa1Zi/edit Dual Monitors vs Side-by-Side We strongly recommend you have dual monitors set-up while attending virtual FOSS Zoom lessons. We will be doing a lot of screen-sharing, and this will make your own interactive sessions less visible, or you will have to make them less than full screen. If you only have one monitor, make sure to exit full screen mode on Zoom and your browser, so you can view everything side-by-side","title":"Account Creation"},{"location":"installation/#required-software","text":"You will need to have the following software installed on your personal computer: Software Notes Web Browser Chrome or Firefox . Text Editor VS Code or SublimeText Attention Windows users Much of what we are going to be teaching is based on open-source software which operates on cloud and is incompatible with Windows OS. Unix-based systems such as Linux Ubuntu and MacOS X , as many scientific tools require a Unix Operating System (OS). There are a number of software that allow Windows users to execute Unix commands, however we recommend the use of Windows Subsystem for Linux (WSL) 2.0 . VS Code is a Microsoft product and integrates seamlessly with Unix systems, we therefore strongly encourage you to install Code on your Windows OS.","title":"Required Software"},{"location":"schedule/","text":"Weekly Schedule \u00b6 Morning session: Thursdays 11:00AM - 1:00PM US Arizona Mountain Standard Time (2:00PM - 4:00PM US Eastern Time) Afternoon session: Thursdays 3:30PM - 5:00PM US Arizona Mountain Standard Time (6:30PM - 8:00PM US Eastern Time) Daylight savings For weeks 9-10, FOSS will be starting at 12:00AM AZ (11:00AM PST) for the morning session and 4:30PM AZ (3:30PM PST) for the afternoon session in an attempt to accomodate attendees that have been affected by daylight savings. Zoom Info \u00b6 Zoom room: https://arizona.zoom.us/j/86152278453 Meeting ID: 861 5227 8453 Calendar \u00b6 Week Date Content Morning Instructor Afternoon Instructor Week 0 Sept 8 pre-FOSS workshop: - Unix shell basics - Git basics Michele Cosi - Week 1 Sept 15 Workshop introduction: - Intro to Open Science Tyson Swetnam Tyson Swetnam Week 2 Sept 22 - Project management - Intro to CyVerse Tyson Swetnam Tyson Swetnam Week 3 Sept 29 Data management : - FAIR data - Data Management Plans - Intro to Data Store Jason Williams Tyson Swetnam/Michele Cosi Week 4 Oct 6 Documentation / Communication : - Internal + External Documentation - Internal + External Communication - GitHub Pages websites Michael Culshaw-Maurer Tyson Swetnam/Michele Cosi Week 5 Oct 13 Version Control - Version control as a philosophy - GitHub functionality Version control everything Jason Williams Michele Cosi Week 6 Oct 20 Reproducibility I : - Software installation - Software management Michele Cosi Michele Cosi Week 7 Oct 27 Reproducibility II : - Brief intro to containers Carlos Lizarraga/Tyson Swetnam Carlos Lizarraga/Tyson Swetnam Week 8 Nov 3 Flex time: - Ask questions - More detail on previous topics Final project work time Week 9 Nov 10 Review, summary and addressing final project inquiries Week 10 Nov 17 Final project presentations","title":"Weekly Schedule"},{"location":"schedule/#weekly-schedule","text":"Morning session: Thursdays 11:00AM - 1:00PM US Arizona Mountain Standard Time (2:00PM - 4:00PM US Eastern Time) Afternoon session: Thursdays 3:30PM - 5:00PM US Arizona Mountain Standard Time (6:30PM - 8:00PM US Eastern Time) Daylight savings For weeks 9-10, FOSS will be starting at 12:00AM AZ (11:00AM PST) for the morning session and 4:30PM AZ (3:30PM PST) for the afternoon session in an attempt to accomodate attendees that have been affected by daylight savings.","title":"Weekly Schedule"},{"location":"schedule/#zoom-info","text":"Zoom room: https://arizona.zoom.us/j/86152278453 Meeting ID: 861 5227 8453","title":"Zoom Info"},{"location":"schedule/#calendar","text":"Week Date Content Morning Instructor Afternoon Instructor Week 0 Sept 8 pre-FOSS workshop: - Unix shell basics - Git basics Michele Cosi - Week 1 Sept 15 Workshop introduction: - Intro to Open Science Tyson Swetnam Tyson Swetnam Week 2 Sept 22 - Project management - Intro to CyVerse Tyson Swetnam Tyson Swetnam Week 3 Sept 29 Data management : - FAIR data - Data Management Plans - Intro to Data Store Jason Williams Tyson Swetnam/Michele Cosi Week 4 Oct 6 Documentation / Communication : - Internal + External Documentation - Internal + External Communication - GitHub Pages websites Michael Culshaw-Maurer Tyson Swetnam/Michele Cosi Week 5 Oct 13 Version Control - Version control as a philosophy - GitHub functionality Version control everything Jason Williams Michele Cosi Week 6 Oct 20 Reproducibility I : - Software installation - Software management Michele Cosi Michele Cosi Week 7 Oct 27 Reproducibility II : - Brief intro to containers Carlos Lizarraga/Tyson Swetnam Carlos Lizarraga/Tyson Swetnam Week 8 Nov 3 Flex time: - Ask questions - More detail on previous topics Final project work time Week 9 Nov 10 Review, summary and addressing final project inquiries Week 10 Nov 17 Final project presentations","title":"Calendar"},{"location":"documentation/githubpages/","text":"GitHub Pages - quick start \u00b6 This is a quick introduction to GitHub Pages , a simple way to use GitHub to set up a small website written in Markdown . This page won't do everything, but you can throw up a basic website, use themes , and extend it. Go to https://github.com/ . Login, or if you don't have an account get one and login. Go to the \"+\" icon on the upper right and select New repository . Enter a name for your repository (e.g. \"profile\"). Enter a description, and leave the repository as public. Select \" Initialize this repository with a README \". If desired select a license. Finally click Create repository . Look for the Settings menu (upper right, next to a \"gear\" icon). Scroll down to GitHub Pages and choose master branch and save your selection. Then Choose a theme and select your theme. You will be asked to Commit changes . Your website will be visible at https://GITHUBUSERNAME.github.io/REPONAME/ . (be sure to change GITHUBUSERNAME to your username, and REPONAME to the name you selected for your repo.) You can edit your website by editing the readme file as desired. Tip You can preview how your Markdown looks using and editor like Markdown Plus .","title":"GitHub Pages - quick start"},{"location":"documentation/githubpages/#github-pages-quick-start","text":"This is a quick introduction to GitHub Pages , a simple way to use GitHub to set up a small website written in Markdown . This page won't do everything, but you can throw up a basic website, use themes , and extend it. Go to https://github.com/ . Login, or if you don't have an account get one and login. Go to the \"+\" icon on the upper right and select New repository . Enter a name for your repository (e.g. \"profile\"). Enter a description, and leave the repository as public. Select \" Initialize this repository with a README \". If desired select a license. Finally click Create repository . Look for the Settings menu (upper right, next to a \"gear\" icon). Scroll down to GitHub Pages and choose master branch and save your selection. Then Choose a theme and select your theme. You will be asked to Commit changes . Your website will be visible at https://GITHUBUSERNAME.github.io/REPONAME/ . (be sure to change GITHUBUSERNAME to your username, and REPONAME to the name you selected for your repo.) You can edit your website by editing the readme file as desired. Tip You can preview how your Markdown looks using and editor like Markdown Plus .","title":"GitHub Pages - quick start"},{"location":"final_project/overview/","text":"Capstone Project Overview \u00b6 The capstone project for FOSS is designed to give you the opportunity to apply some of the skills and ideas from the rest of the course to one of your own projects, get support and feedback from your peers, and share your experiences with the rest of the attendees. Objectives \u00b6 \"Level up\" the openness of an existing or planned project Apply a skill or concept from FOSS Discuss your applied skills/concepts with groupmates Determine possible next steps to \"level up\" your project Share your experiences with the rest of the attendees Description \u00b6 For the project, you will choose one (or more) skill(s) or concept(s) from FOSS and apply it to one of your own projects. For instance, you could write a Data Management Plan for an upcoming project proposal, or you could reorganize an existing project and put all the code onto GitHub. You should try to identify what level your project is currently at for a given topic, and attempt to move up a level. This means you can try out a skill or idea you've never used before, or go to a more advanced level for something you already do. You should have your skill/topic chosen by Week 8 , and you will then enter your name / topic into a spreadsheet (shared during the week 8 session). We'll use this to put together small groups. Week 8 will be your time to work on your personal project and ask us questions. With the information you shared in the spreadsheets, we will identify potential groups of people working on a similar topic to yours. Groups will be revealed on Week 9 . During Week 9 , you will meet with your group to: discuss your experiences applying your skill or concept, give each other help with sticking points you may have encountered put together a short (5-10 minute) presentation to be given to the rest of the groups during Week 10 . Discussion Guide \u00b6 Your small group presentation should focus on the challenges and tips you may have for other FOSS attendees who want to utilize your skill or concept in the future. Here are some prompts that you should address during your presentation: What was the general topic or skill that members of your group worked on? What are some challenges you encountered while working on your projects? Were you able to overcome these challenges? If so, how? Where did you look for help? Do any roadblocks remain? How might you try to overcome them? Are there any new things you learned while working on your project? Did you end up using any new or different tools? What are some tips you might have for other FOSS attendees who want to work on the same topic/skill in the future? Are there any pitfalls to avoid? What things do you want to do next? How might you \"level up\" in the current topic/skill compared to your project's current state? Are there any other FOSS skills that you want to tackle next? If so, how might they integrate with the topic/skill you focused on for your project? Timeline \u00b6 Week 8: finalize topic choice and share it with us Between Week 8 and Week 9: work on your project individually Week 9: group discussions with members with a similar topic and presentation prep Week 10: group presentations Examples \u00b6 Links to previous' year capstone projects will be posted in the FOSS HackMD. Stay tuned!","title":"Capstone Project Overview"},{"location":"final_project/overview/#capstone-project-overview","text":"The capstone project for FOSS is designed to give you the opportunity to apply some of the skills and ideas from the rest of the course to one of your own projects, get support and feedback from your peers, and share your experiences with the rest of the attendees.","title":"Capstone Project Overview"},{"location":"final_project/overview/#objectives","text":"\"Level up\" the openness of an existing or planned project Apply a skill or concept from FOSS Discuss your applied skills/concepts with groupmates Determine possible next steps to \"level up\" your project Share your experiences with the rest of the attendees","title":"Objectives"},{"location":"final_project/overview/#description","text":"For the project, you will choose one (or more) skill(s) or concept(s) from FOSS and apply it to one of your own projects. For instance, you could write a Data Management Plan for an upcoming project proposal, or you could reorganize an existing project and put all the code onto GitHub. You should try to identify what level your project is currently at for a given topic, and attempt to move up a level. This means you can try out a skill or idea you've never used before, or go to a more advanced level for something you already do. You should have your skill/topic chosen by Week 8 , and you will then enter your name / topic into a spreadsheet (shared during the week 8 session). We'll use this to put together small groups. Week 8 will be your time to work on your personal project and ask us questions. With the information you shared in the spreadsheets, we will identify potential groups of people working on a similar topic to yours. Groups will be revealed on Week 9 . During Week 9 , you will meet with your group to: discuss your experiences applying your skill or concept, give each other help with sticking points you may have encountered put together a short (5-10 minute) presentation to be given to the rest of the groups during Week 10 .","title":"Description"},{"location":"final_project/overview/#discussion-guide","text":"Your small group presentation should focus on the challenges and tips you may have for other FOSS attendees who want to utilize your skill or concept in the future. Here are some prompts that you should address during your presentation: What was the general topic or skill that members of your group worked on? What are some challenges you encountered while working on your projects? Were you able to overcome these challenges? If so, how? Where did you look for help? Do any roadblocks remain? How might you try to overcome them? Are there any new things you learned while working on your project? Did you end up using any new or different tools? What are some tips you might have for other FOSS attendees who want to work on the same topic/skill in the future? Are there any pitfalls to avoid? What things do you want to do next? How might you \"level up\" in the current topic/skill compared to your project's current state? Are there any other FOSS skills that you want to tackle next? If so, how might they integrate with the topic/skill you focused on for your project?","title":"Discussion Guide"},{"location":"final_project/overview/#timeline","text":"Week 8: finalize topic choice and share it with us Between Week 8 and Week 9: work on your project individually Week 9: group discussions with members with a similar topic and presentation prep Week 10: group presentations","title":"Timeline"},{"location":"final_project/overview/#examples","text":"Links to previous' year capstone projects will be posted in the FOSS HackMD. Stay tuned!","title":"Examples"}]}